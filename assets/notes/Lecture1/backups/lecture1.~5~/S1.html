<!DOCTYPE html><html lang="en-GB">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>1. BAYESIAN INFERENCE ‣ Lecture 1: Foundations in Deterministic and Stochastic Optimization</title>
<!--Generated on Mon Mar 18 14:03:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<link rel="stylesheet" href="ltx-amsart.css" type="text/css">
<link rel="stylesheet" href="bookml/gitbook/css/style.css" type="text/css">
<link rel="stylesheet" href="bookml/gitbook/css/plugin-table.css" type="text/css">
<link rel="stylesheet" href="bookml/gitbook/css/plugin-bookdown.css" type="text/css">
<link rel="stylesheet" href="bookml/gitbook/css/plugin-fontsettings.css" type="text/css">
<link rel="stylesheet" href="bookml/CSS/style.css" type="text/css">
<link rel="stylesheet" href="bookml/CSS/style.gitbook,plain.css" type="text/css">
<link rel="stylesheet" href="bookml/CSS/style.gitbook.css" type="text/css">
<link rel="stylesheet" href="bmluser/lecture1.colors.gitbook.css" type="text/css">
<link rel="stylesheet" href="bmluser/template.colors.gitbook.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<script src="https://code.jquery.com/jquery-3.6.0.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<link rel="up" href="index.html" title="Lecture 1: Foundations in Deterministic and Stochastic Optimization">
<link rel="start" href="index.html" title="Lecture 1: Foundations in Deterministic and Stochastic Optimization">
<link rel="prev" href="index.html" title="Lecture 1: Foundations in Deterministic and Stochastic Optimization">
</head>
<body>
<div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">
<a href="#bml-main-content" tabindex="0" class="bml-skip-to-content">Skip to content.</a>
<div class="book-summary"><nav class="ltx_page_navbar">
<ul class="ltx_toclist summary">
<li>
<a href="index.html" title="" class="ltx_ref" rel="start">Lecture 1: Foundations in Deterministic and Stochastic Optimization</a>
</li>
<li class="divider">
<li data-level="1" data-path="" class="ltx_tocentry ltx_tocentry_section ltx_ref_self  chapter active">
<a href="" title="In Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><b><span class="ltx_tag ltx_tag_ref">1 </span></b>Bayesian Inference</a>
<ol class="ltx_toclist ltx_toclist_section">
<li data-level="1.1" data-path="" class="ltx_tocentry ltx_tocentry_subsection  chapter "><a href="#SS1" title="In 1. BAYESIAN INFERENCE ‣ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><b><span class="ltx_tag ltx_tag_ref">1.1 </span></b>From Inference to Learning</a></li>
</ol>
</li>
</ul>
</nav></div>
<div class="ltx_page_main book-body fixed">
<div class="body-inner">
<div class="book-header fixed" role="navigation"><h1>BAYESIAN INFERENCE</h1></div>
<div class="page-wrapper" tabindex="-1" role="main">
<div class="ltx_page_content page-inner" id="bml-main-content">
<section class="ltx_section ltx_authors_1line ltx_leqno normal" lang="en-GB">
<div class="section level1">

<h1 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section header-section-number">1</span>. BAYESIAN INFERENCE</h1>

<div id="p1" class="ltx_para">
<p class="ltx_p">One of the most fundamental problems in statistics, signal processing, and machine learning, is the <em class="ltx_emph ltx_font_italic">inference</em> problem, where we wish to construct an estimate of some random quantity of interest <math id="p1.m1" class="ltx_Math" alttext="\boldsymbol{\gamma}\in\mathds{R}^{M_{\boldsymbol{\gamma}}}" display="inline"><semantics><mrow><mi>𝜸</mi><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>M</mi><mi>𝜸</mi></msub></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{\gamma}\in\mathds{R}^{M_{\boldsymbol{\gamma}}}</annotation></semantics></math>, given observations of a related random variable <math id="p1.m2" class="ltx_Math" alttext="\boldsymbol{h}\in\mathds{R}^{M_{\boldsymbol{h}}}" display="inline"><semantics><mrow><mi>𝒉</mi><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>M</mi><mi>𝒉</mi></msub></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{h}\in\mathds{R}^{M_{\boldsymbol{h}}}</annotation></semantics></math>. The quantity of interest <math id="p1.m3" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>𝜸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math> may take on continuous or discrete values, and is referred to as the “dependent variable,” “state of nature,” “class,” or “label” depending on the application. We will most commonly refer to it as the label. We will refer to the observed random variable <math id="p1.m4" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>𝒉</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math> generally as the feature, although in some applications it is known as “regressor” or “observation”.</p>
</div>
<div id="p2" class="ltx_para">
<p class="ltx_p">If we are provided with the conditional distribution <math id="p2.m1" class="ltx_Math" alttext="f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|h)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>𝜸</mi><mo fence="false">|</mo><mi>𝒉</mi></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>γ</mi><mo fence="false">|</mo><mi>h</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|h)</annotation></semantics></math> along with a single realization of the feature <math id="p2.m2" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>𝒉</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>, it is quite natural to estimate <math id="p2.m3" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>𝜸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math> as the most likely outcome given <math id="p2.m4" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>𝒉</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>. We can express this formally as:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E1.m1" class="ltx_Math" alttext="\displaystyle{\gamma^{\star}}\triangleq\arg\max_{{\gamma}\in\Gamma}f_{%
\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|h)" display="inline"><semantics><mrow><msup><mi>γ</mi><mo>⋆</mo></msup><mo>≜</mo><mrow><mrow><mi>arg</mi><mo lspace="0.167em">⁡</mo><mrow><munder><mi>max</mi><mrow><mi>γ</mi><mo>∈</mo><mi mathvariant="normal">Γ</mi></mrow></munder><mo lspace="0.167em">⁡</mo><msub><mi>f</mi><mrow><mi>𝜸</mi><mo fence="false">|</mo><mi>𝒉</mi></mrow></msub></mrow></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>γ</mi><mo fence="false">|</mo><mi>h</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle{\gamma^{\star}}\triangleq\arg\max_{{\gamma}\in\Gamma}f_{%
\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|h)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">The conditional distribution <math id="p2.m5" class="ltx_Math" alttext="f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|h)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>𝜸</mi><mo fence="false">|</mo><mi>𝒉</mi></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>γ</mi><mo fence="false">|</mo><mi>h</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|h)</annotation></semantics></math> is frequently referred to as the <em class="ltx_emph ltx_font_italic">posterior distribution</em> of <math id="p2.m6" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>𝜸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math>, making <math id="p2.m7" class="ltx_Math" alttext="{\gamma^{\star}}" display="inline"><semantics><msup><mi>γ</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">{\gamma^{\star}}</annotation></semantics></math> the <em class="ltx_emph ltx_font_italic">maximum a posteriori (MAP) estimate</em> of <math id="p2.m8" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>𝜸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math> given <math id="p2.m9" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>𝒉</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>. We note that it is common in the literature on estimation theory to the employ hat-notation to refer to estimates of a random quantity based on data. In this sense, we could have opted to denote the optimal solution to (<a href="#E1" title="In 1. BAYESIAN INFERENCE ‣ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) by <math id="p2.m10" class="ltx_Math" alttext="\widehat{\gamma}" display="inline"><semantics><mover accent="true"><mi>γ</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\widehat{\gamma}</annotation></semantics></math>, rather than <math id="p2.m11" class="ltx_Math" alttext="\gamma^{\star}" display="inline"><semantics><msup><mi>γ</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\gamma^{\star}</annotation></semantics></math>. Nevertheless, as we transition from inference to learning and optimization, we will increasingly interpret estimates as solutions to generic optimization problems. In this context, it will be appropriate to employ the <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">⋆</span></sup>-notation to refer to an optimal solution. To preserve consistency throughout this text, we opt to use the <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">⋆</span></sup>-notation from the onset, with the understanding that within the Bayesian framework (<a href="#E1" title="In 1. BAYESIAN INFERENCE ‣ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) the optimal solution <math id="p2.m14" class="ltx_Math" alttext="\gamma^{\star}" display="inline"><semantics><msup><mi>γ</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\gamma^{\star}</annotation></semantics></math> carries the interpretation of an estimate. Finally, we note that (<a href="#E1" title="In 1. BAYESIAN INFERENCE ‣ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) is formulated for a particular realization <math id="p2.m15" class="ltx_Math" alttext="h" display="inline"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math> of the random variable <math id="p2.m16" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>𝒉</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>. As we will see, most MAP estimators are derived for a particular realization of the feature vector, resulting in a deterministic estimate <math id="p2.m17" class="ltx_Math" alttext="\gamma^{\star}" display="inline"><semantics><msup><mi>γ</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\gamma^{\star}</annotation></semantics></math> as a function of the realization <math id="p2.m18" class="ltx_Math" alttext="h" display="inline"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>. We can also regard the MAP solution as a random variable, denoted in boldface notation <math id="p2.m19" class="ltx_Math" alttext="\boldsymbol{\gamma}^{\star}" display="inline"><semantics><msup><mi>𝜸</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\boldsymbol{\gamma}^{\star}</annotation></semantics></math>, when it is viewed as a function of the random observation <math id="p2.m20" class="ltx_Math" alttext="{\boldsymbol{h}}" display="inline"><semantics><mi>𝒉</mi><annotation encoding="application/x-tex">{\boldsymbol{h}}</annotation></semantics></math>:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E2.m1" class="ltx_Math" alttext="\displaystyle\boldsymbol{\gamma}^{\star}\triangleq\arg\max_{{\gamma}\in\Gamma}%
f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|\boldsymbol{h})" display="inline"><semantics><mrow><msup><mi>𝜸</mi><mo>⋆</mo></msup><mo>≜</mo><mrow><mrow><mi>arg</mi><mo lspace="0.167em">⁡</mo><mrow><munder><mi>max</mi><mrow><mi>γ</mi><mo>∈</mo><mi mathvariant="normal">Γ</mi></mrow></munder><mo lspace="0.167em">⁡</mo><msub><mi>f</mi><mrow><mi>𝜸</mi><mo fence="false">|</mo><mi>𝒉</mi></mrow></msub></mrow></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>γ</mi><mo fence="false">|</mo><mi>𝒉</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\boldsymbol{\gamma}^{\star}\triangleq\arg\max_{{\gamma}\in\Gamma}%
f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|\boldsymbol{h})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">where the posterior distribution <math id="p2.m21" class="ltx_Math" alttext="f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|\boldsymbol{h})" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>𝜸</mi><mo fence="false">|</mo><mi>𝒉</mi></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>γ</mi><mo fence="false">|</mo><mi>𝒉</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|\boldsymbol{h})</annotation></semantics></math> is now a function of the random variable <math id="p2.m22" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>𝒉</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>, rather than its realization <math id="p2.m23" class="ltx_Math" alttext="h" display="inline"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>.</p>
</div>
<div id="Thmremark1" class="ltx_theorem ltx_theorem_remark">

<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem header-section-number"><span class="ltx_text ltx_font_italic">Remark 1.1</span></span><span class="ltx_text ltx_font_italic"> </span>(<span class="ltx_text ltx_font_bold"> Bold font indicates random variables</span>)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div id="Thmremark1.p1" class="ltx_para">
<p class="ltx_p">We note that we employ bold font to denote a random variables, for example <math id="Thmremark1.p1.m1" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>𝒉</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>, while regular font denotes its realization or a deterministic quanity, as in <math id="Thmremark1.p1.m2" class="ltx_Math" alttext="h" display="inline"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>. As a general rule, we use lowercase fonts to denote vectors, while uppercase letters denote matrices. In this way, a random matrix would be denoted by <math id="Thmremark1.p1.m3" class="ltx_Math" alttext="\boldsymbol{H}" display="inline"><semantics><mi>𝑯</mi><annotation encoding="application/x-tex">\boldsymbol{H}</annotation></semantics></math>, while its realization would correspond to <math id="Thmremark1.p1.m4" class="ltx_Math" alttext="H" display="inline"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>.</p>
</div>
</div>
<figure id="F1" class="ltx_figure"><div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;"><img src="" id="F1.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption"></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span><span class="ltx_text" style="font-size:90%;">A general model underlying most inference problems. The top row illustrates the generative model for the observations <math id="F1.m2" class="ltx_Math" alttext="{\boldsymbol{h}}" display="inline"><semantics><mi>𝒉</mi><annotation encoding="application/x-tex">{\boldsymbol{h}}</annotation></semantics></math>, while the bottom row illustrates the Bayesian inference formulation for recovering the label variable.</span></figcaption>
</figure>
<section id="SS1" class="ltx_subsection normal">
<div class="section level2">

<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection header-section-number">1.1</span>. From Inference to Learning</h2>

<div id="SS1.p1" class="ltx_para">
<p class="ltx_p">Our previous discussion leading has uncovered how to perform MAP inference of a random quantity of interest <math id="SS1.p1.m1" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>𝜸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math> given observations <math id="SS1.p1.m2" class="ltx_Math" alttext="\{\boldsymbol{h}_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒉</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n}\}_{n=1}^{N}</annotation></semantics></math>, models <math id="SS1.p1.m3" class="ltx_math_unparsed" alttext="f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>𝒉</mi><mo fence="false">|</mo><mi>𝜸</mi></mrow></msub><mrow><mo>(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>γ</mi><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)</annotation></semantics></math>, and prior information <math id="SS1.p1.m4" class="ltx_Math" alttext="f_{\boldsymbol{\gamma}}(\gamma)" display="inline"><semantics><mrow><msub><mi>f</mi><mi>𝜸</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>γ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{\gamma}}(\gamma)</annotation></semantics></math>. In many practical situations, we are not provided with prior knowledge about the model relating the label <math id="SS1.p1.m5" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>𝜸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math> with the feature <math id="SS1.p1.m6" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>𝒉</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>. Instead, we need to estimate <math id="SS1.p1.m7" class="ltx_math_unparsed" alttext="f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>𝒉</mi><mo fence="false">|</mo><mi>𝜸</mi></mrow></msub><mrow><mo>(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>γ</mi><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)</annotation></semantics></math> from data before performing inference. We refer to the process of estimating statistical properties of data needed for inference, such as <math id="SS1.p1.m8" class="ltx_math_unparsed" alttext="f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>𝒉</mi><mo fence="false">|</mo><mi>𝜸</mi></mrow></msub><mrow><mo>(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>γ</mi><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)</annotation></semantics></math>, as <em class="ltx_emph ltx_font_italic">learning</em>. As we will see in this section, the learning of models can be formalized using a Bayesian MAP framework analogous to (<a href="#E1" title="In 1. BAYESIAN INFERENCE ‣ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). To this end, we will assume that the conditional likelihood <math id="SS1.p1.m9" class="ltx_math_unparsed" alttext="f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>𝒉</mi><mo fence="false">|</mo><mi>𝜸</mi></mrow></msub><mrow><mo>(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>γ</mi><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)</annotation></semantics></math> is parameterized by a learnable parameter <math id="SS1.p1.m10" class="ltx_Math" alttext="w\in\mathds{R}^{M_{w}}" display="inline"><semantics><mrow><mi>w</mi><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>M</mi><mi>w</mi></msub></msup></mrow><annotation encoding="application/x-tex">w\in\mathds{R}^{M_{w}}</annotation></semantics></math> and write instead <math id="SS1.p1.m11" class="ltx_math_unparsed" alttext="f_{\boldsymbol{h}|\boldsymbol{\gamma},\boldsymbol{w}}\left(\cdot|{\gamma},w\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>𝒉</mi><mo fence="false">|</mo><mrow><mi>𝜸</mi><mo>,</mo><mi>𝒘</mi></mrow></mrow></msub><mrow><mo>(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>γ</mi><mo>,</mo><mi>w</mi><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h}|\boldsymbol{\gamma},\boldsymbol{w}}\left(\cdot|{\gamma},w\right)</annotation></semantics></math>. Under this parameterization, learning the conditional distribution of <math id="SS1.p1.m12" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>𝜸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math> given <math id="SS1.p1.m13" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>𝒉</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math> is equivalent to learning the parameter <math id="SS1.p1.m14" class="ltx_Math" alttext="\boldsymbol{w}" display="inline"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">\boldsymbol{w}</annotation></semantics></math> that parameterizes <math id="SS1.p1.m15" class="ltx_math_unparsed" alttext="f_{\boldsymbol{h}|\boldsymbol{\gamma},\boldsymbol{w}}\left(\cdot|{\gamma},w\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>𝒉</mi><mo fence="false">|</mo><mrow><mi>𝜸</mi><mo>,</mo><mi>𝒘</mi></mrow></mrow></msub><mrow><mo>(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>γ</mi><mo>,</mo><mi>w</mi><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h}|\boldsymbol{\gamma},\boldsymbol{w}}\left(\cdot|{\gamma},w\right)</annotation></semantics></math>.</p>
</div>
<div id="SS1.p2" class="ltx_para">
<p class="ltx_p">To formulate a procedure for learning <math id="SS1.p2.m1" class="ltx_Math" alttext="\boldsymbol{w}" display="inline"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">\boldsymbol{w}</annotation></semantics></math> from pairs <math id="SS1.p2.m2" class="ltx_Math" alttext="\{\boldsymbol{h},\boldsymbol{\gamma}\}" display="inline"><semantics><mrow><mo stretchy="false">{</mo><mi>𝒉</mi><mo>,</mo><mi>𝜸</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\boldsymbol{h},\boldsymbol{\gamma}\}</annotation></semantics></math> we mirror the argument in Section <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:sec:estimates_using_a_batch_of_samples</span>. Suppose the model <math id="SS1.p2.m3" class="ltx_Math" alttext="{\boldsymbol{w}}" display="inline"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">{\boldsymbol{w}}</annotation></semantics></math> is sampled once, yielding the realization <math id="SS1.p2.m4" class="ltx_Math" alttext="w^{o}" display="inline"><semantics><msup><mi>w</mi><mi>o</mi></msup><annotation encoding="application/x-tex">w^{o}</annotation></semantics></math>. The <em class="ltx_emph ltx_font_italic">training data</em> <math id="SS1.p2.m5" class="ltx_Math" alttext="\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒉</mi><mi>n</mi></msub><mo>,</mo><msub><mi>𝜸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}</annotation></semantics></math> is sampled <math id="SS1.p2.m6" class="ltx_Math" alttext="N" display="inline"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> times, where each pair <math id="SS1.p2.m7" class="ltx_Math" alttext="\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}" display="inline"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒉</mi><mi>n</mi></msub><mo>,</mo><msub><mi>𝜸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}</annotation></semantics></math> is sampled from <math id="SS1.p2.m8" class="ltx_Math" alttext="f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left(h,{\gamma}|w^{o}\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>𝒉</mi><mo>,</mo><mrow><mi>𝜸</mi><mo fence="false">|</mo><mi>𝒘</mi></mrow></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mi>h</mi><mo>,</mo><mrow><mi>γ</mi><mo fence="false">|</mo><msup><mi>w</mi><mi>o</mi></msup></mrow><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left(h,{\gamma}|w^{o}\right)</annotation></semantics></math>. If we suppose that feature-label pairs <math id="SS1.p2.m9" class="ltx_Math" alttext="\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}" display="inline"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒉</mi><mi>n</mi></msub><mo>,</mo><msub><mi>𝜸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}</annotation></semantics></math> are identically and independently distributed after conditioning on the parameterization <math id="SS1.p2.m10" class="ltx_Math" alttext="{\boldsymbol{w}}" display="inline"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">{\boldsymbol{w}}</annotation></semantics></math>, we can factorize:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="Ex1.m1" class="ltx_Math" alttext="\displaystyle f_{\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}|%
\boldsymbol{w}}\left(\left\{{h}_{n},\gamma_{n}\right\}_{n=1}^{N}|w\right)%
\stackrel{{\scriptstyle(a)}}{{=}}" display="inline"><semantics><mrow><mrow><msub><mi>f</mi><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒉</mi><mi>n</mi></msub><mo>,</mo><msub><mi>𝜸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo fence="false">|</mo><mi>𝒘</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msubsup><mrow><mo>{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>n</mi></msub><mo>}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow><mover><mo>=</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mover><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle f_{\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}|%
\boldsymbol{w}}\left(\left\{{h}_{n},\gamma_{n}\right\}_{n=1}^{N}|w\right)%
\stackrel{{\scriptstyle(a)}}{{=}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="Ex1.m2" class="ltx_Math" alttext="\displaystyle\&gt;\prod_{n=1}^{N}f_{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}|%
\boldsymbol{w}}\left({h}_{n},{\gamma}_{n}|w\right)" display="inline"><semantics><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∏</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>f</mi><mrow><msub><mi>𝒉</mi><mi>n</mi></msub><mo>,</mo><mrow><msub><mi>𝜸</mi><mi>n</mi></msub><mo fence="false">|</mo><mi>𝒘</mi></mrow></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><mrow><msub><mi>γ</mi><mi>n</mi></msub><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\prod_{n=1}^{N}f_{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}|%
\boldsymbol{w}}\left({h}_{n},{\gamma}_{n}|w\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E3.m1" class="ltx_Math" alttext="\displaystyle\stackrel{{\scriptstyle(b)}}{{=}}" display="inline"><semantics><mover><mo>=</mo><mrow><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mover><annotation encoding="application/x-tex">\displaystyle\stackrel{{\scriptstyle(b)}}{{=}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="E3.m2" class="ltx_Math" alttext="\displaystyle\&gt;\prod_{n=1}^{N}f_{\boldsymbol{h},\boldsymbol{\gamma}|%
\boldsymbol{w}}\left({h}_{n},{\gamma}_{n}|w\right)" display="inline"><semantics><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∏</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>f</mi><mrow><mi>𝒉</mi><mo>,</mo><mrow><mi>𝜸</mi><mo fence="false">|</mo><mi>𝒘</mi></mrow></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><mrow><msub><mi>γ</mi><mi>n</mi></msub><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\prod_{n=1}^{N}f_{\boldsymbol{h},\boldsymbol{\gamma}|%
\boldsymbol{w}}\left({h}_{n},{\gamma}_{n}|w\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">Step <math id="SS1.p2.m11" class="ltx_Math" alttext="(a)" display="inline"><semantics><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(a)</annotation></semantics></math> holds by conditional independence and <math id="SS1.p2.m12" class="ltx_Math" alttext="(b)" display="inline"><semantics><mrow><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(b)</annotation></semantics></math> holds by identical distribution of the pairs of random variables <math id="SS1.p2.m13" class="ltx_Math" alttext="\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}" display="inline"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒉</mi><mi>n</mi></msub><mo>,</mo><msub><mi>𝜸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}</annotation></semantics></math>. We can then define the MAP estimate of the weight vector <math id="SS1.p2.m14" class="ltx_Math" alttext="w" display="inline"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math> as:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="Ex2.m1" class="ltx_Math" alttext="\displaystyle w^{\star}\triangleq" display="inline"><semantics><mrow><msup><mi>w</mi><mo>⋆</mo></msup><mo>≜</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle w^{\star}\triangleq</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="Ex2.m2" class="ltx_Math" alttext="\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}f_{\boldsymbol{w}|\{%
\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}}\left(w|\left\{{h}_{n},%
\gamma_{n}\right\}_{n=1}^{N}\right)" display="inline"><semantics><mrow><mrow><mi>arg</mi><mo lspace="0.167em">⁡</mo><mrow><munder><mi>max</mi><mrow><mi>w</mi><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>M</mi><mi>w</mi></msub></msup></mrow></munder><mo lspace="0.167em">⁡</mo><msub><mi>f</mi><mrow><mi>𝒘</mi><mo fence="false">|</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒉</mi><mi>n</mi></msub><mo>,</mo><msub><mi>𝜸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow></msub></mrow></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo fence="false">|</mo><msubsup><mrow><mo>{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>n</mi></msub><mo>}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}f_{\boldsymbol{w}|\{%
\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}}\left(w|\left\{{h}_{n},%
\gamma_{n}\right\}_{n=1}^{N}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="Ex3.m1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="Ex3.m2" class="ltx_Math" alttext="\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}\frac{f_{\{\boldsymbol{h}_{n},%
\boldsymbol{\gamma}_{n}\}_{n=1}^{N}|\boldsymbol{w}}\left(\left\{{h}_{n},\gamma%
_{n}\right\}_{n=1}^{N}|w\right)\times f_{\boldsymbol{w}}(w)}{f_{\{\boldsymbol{%
h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}}\left(\left\{{h}_{n},\gamma_{n}%
\right\}_{n=1}^{N}\right)}" display="inline"><semantics><mrow><mi>arg</mi><mo lspace="0.167em">⁡</mo><mrow><munder><mi>max</mi><mrow><mi>w</mi><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>M</mi><mi>w</mi></msub></msup></mrow></munder><mo lspace="0.167em">⁡</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><mrow><msub><mi>f</mi><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒉</mi><mi>n</mi></msub><mo>,</mo><msub><mi>𝜸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo fence="false">|</mo><mi>𝒘</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msubsup><mrow><mo>{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>n</mi></msub><mo>}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo fence="false">|</mo><mi>w</mi></mrow><mo rspace="0.055em">)</mo></mrow></mrow><mo rspace="0.222em">×</mo><msub><mi>f</mi><mi>𝒘</mi></msub></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>f</mi><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒉</mi><mi>n</mi></msub><mo>,</mo><msub><mi>𝜸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></msub><mo>⁢</mo><mrow><mo>(</mo><msubsup><mrow><mo>{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>n</mi></msub><mo>}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}\frac{f_{\{\boldsymbol{h}_{n},%
\boldsymbol{\gamma}_{n}\}_{n=1}^{N}|\boldsymbol{w}}\left(\left\{{h}_{n},\gamma%
_{n}\right\}_{n=1}^{N}|w\right)\times f_{\boldsymbol{w}}(w)}{f_{\{\boldsymbol{%
h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}}\left(\left\{{h}_{n},\gamma_{n}%
\right\}_{n=1}^{N}\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="Ex4.m1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="Ex4.m2" class="ltx_Math" alttext="\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}{f_{\{\boldsymbol{h}_{n},%
\boldsymbol{\gamma}_{n}\}_{n=1}^{N}|\boldsymbol{w}}\left(\left\{{h}_{n},\gamma%
_{n}\right\}_{n=1}^{N}|w\right)\times f_{\boldsymbol{w}}(w)}" display="inline"><semantics><mrow><mrow><mrow><mrow><mi>arg</mi><mo lspace="0.167em">⁡</mo><mrow><munder><mi>max</mi><mrow><mi>w</mi><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>M</mi><mi>w</mi></msub></msup></mrow></munder><mo lspace="0.167em">⁡</mo><msub><mi>f</mi><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒉</mi><mi>n</mi></msub><mo>,</mo><msub><mi>𝜸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo fence="false">|</mo><mi>𝒘</mi></mrow></msub></mrow></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><msubsup><mrow><mo>{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>n</mi></msub><mo>}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo fence="false">|</mo><mi>w</mi></mrow><mo rspace="0.055em">)</mo></mrow></mrow><mo rspace="0.222em">×</mo><msub><mi>f</mi><mi>𝒘</mi></msub></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}{f_{\{\boldsymbol{h}_{n},%
\boldsymbol{\gamma}_{n}\}_{n=1}^{N}|\boldsymbol{w}}\left(\left\{{h}_{n},\gamma%
_{n}\right\}_{n=1}^{N}|w\right)\times f_{\boldsymbol{w}}(w)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E4.m1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="E4.m2" class="ltx_Math" alttext="\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}\left(\prod_{n=1}^{N}f_{%
\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left({h}_{n},{\gamma}_{n}|w%
\right)\right)\times f_{\boldsymbol{w}}(w)" display="inline"><semantics><mrow><mrow><mi>arg</mi><mo lspace="0.167em">⁡</mo><mrow><mrow><munder><mi>max</mi><mrow><mi>w</mi><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>M</mi><mi>w</mi></msub></msup></mrow></munder><mo>⁡</mo><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∏</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>f</mi><mrow><mi>𝒉</mi><mo>,</mo><mrow><mi>𝜸</mi><mo fence="false">|</mo><mi>𝒘</mi></mrow></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><mrow><msub><mi>γ</mi><mi>n</mi></msub><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace="0.055em">)</mo></mrow></mrow><mo rspace="0.222em">×</mo><msub><mi>f</mi><mi>𝒘</mi></msub></mrow></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}\left(\prod_{n=1}^{N}f_{%
\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left({h}_{n},{\gamma}_{n}|w%
\right)\right)\times f_{\boldsymbol{w}}(w)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">Following the same argument that led to (<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:eq:minimize_log_likelihoods</span>), we arrive at:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E5.m1" class="ltx_Math" alttext="\displaystyle w^{\star}=" display="inline"><semantics><mrow><msup><mi>w</mi><mo>⋆</mo></msup><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle w^{\star}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="E5.m2" class="ltx_Math" alttext="\displaystyle\&gt;\arg\min_{w\in\mathds{R}^{M}}\left\{-\frac{1}{N}\sum_{n=1}^{N}%
\log f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left({h}_{n},{%
\gamma}_{n}|w\right)-\frac{1}{N}\log f_{\boldsymbol{w}}(w)\right\}" display="inline"><semantics><mrow><mi>arg</mi><mo lspace="0.167em">⁡</mo><mrow><munder><mi>min</mi><mrow><mi>w</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>M</mi></msup></mrow></munder><mo>⁡</mo><mrow><mo>{</mo><mrow><mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>f</mi><mrow><mi>𝒉</mi><mo>,</mo><mrow><mi>𝜸</mi><mo fence="false">|</mo><mi>𝒘</mi></mrow></mrow></msub></mrow><mo>⁢</mo><mrow><mo>(</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><mrow><msub><mi>γ</mi><mi>n</mi></msub><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo lspace="0.167em">⁢</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>f</mi><mi>𝒘</mi></msub></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\arg\min_{w\in\mathds{R}^{M}}\left\{-\frac{1}{N}\sum_{n=1}^{N}%
\log f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left({h}_{n},{%
\gamma}_{n}|w\right)-\frac{1}{N}\log f_{\boldsymbol{w}}(w)\right\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="Thmremark2" class="ltx_theorem ltx_theorem_remark">

<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem header-section-number"><span class="ltx_text ltx_font_italic">Remark 1.2</span></span><span class="ltx_text ltx_font_italic"> </span>(<span class="ltx_text ltx_font_bold">Distinction between the true model and the MAP estimate</span>)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div id="Thmremark2.p1" class="ltx_para">
<p class="ltx_p">Observe that we make a deliberate distinction between the <em class="ltx_emph ltx_font_italic">true</em> model <math id="Thmremark2.p1.m1" class="ltx_Math" alttext="w^{o}" display="inline"><semantics><msup><mi>w</mi><mi>o</mi></msup><annotation encoding="application/x-tex">w^{o}</annotation></semantics></math>, which parameterizes the distribution <math id="Thmremark2.p1.m2" class="ltx_Math" alttext="f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left(h,{\gamma}|w^{o}\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>𝒉</mi><mo>,</mo><mrow><mi>𝜸</mi><mo fence="false">|</mo><mi>𝒘</mi></mrow></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mi>h</mi><mo>,</mo><mrow><mi>γ</mi><mo fence="false">|</mo><msup><mi>w</mi><mi>o</mi></msup></mrow><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left(h,{\gamma}|w^{o}\right)</annotation></semantics></math> from which the samples <math id="Thmremark2.p1.m3" class="ltx_Math" alttext="\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒉</mi><mi>n</mi></msub><mo>,</mo><msub><mi>𝜸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}</annotation></semantics></math> are generated, and the MAP <em class="ltx_emph ltx_font_italic">estimate</em> <math id="Thmremark2.p1.m4" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math>, which maximizes the posterior distribution of <math id="Thmremark2.p1.m5" class="ltx_Math" alttext="{\boldsymbol{w}}" display="inline"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">{\boldsymbol{w}}</annotation></semantics></math> after observing the samples <math id="Thmremark2.p1.m6" class="ltx_Math" alttext="\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒉</mi><mi>n</mi></msub><mo>,</mo><msub><mi>𝜸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}</annotation></semantics></math>. In general, there will be a difference between the MAP model <math id="Thmremark2.p1.m7" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> and the true model <math id="Thmremark2.p1.m8" class="ltx_Math" alttext="w^{o}" display="inline"><semantics><msup><mi>w</mi><mi>o</mi></msup><annotation encoding="application/x-tex">w^{o}</annotation></semantics></math>. The expectation is that as the size of the data set <math id="Thmremark2.p1.m9" class="ltx_Math" alttext="N" display="inline"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> grows, the MAP model <math id="Thmremark2.p1.m10" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> will become increasingly accurate and approach <math id="Thmremark2.p1.m11" class="ltx_Math" alttext="w^{o}" display="inline"><semantics><msup><mi>w</mi><mi>o</mi></msup><annotation encoding="application/x-tex">w^{o}</annotation></semantics></math> as <math id="Thmremark2.p1.m12" class="ltx_Math" alttext="N\to\infty" display="inline"><semantics><mrow><mi>N</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">N\to\infty</annotation></semantics></math>. We will verify that this is the case further below. ∎</p>
</div>
</div>
<div id="SS1.p3" class="ltx_para">
<p class="ltx_p">Note that in order to be able to <em class="ltx_emph ltx_font_italic">learn</em> an optimal parameterization <math id="SS1.p3.m1" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> according to (<a href="#E5" title="In 1.1. From Inference to Learning ‣ 1. BAYESIAN INFERENCE ‣ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), we need to be able to collect a batch of feature-label pairs <math id="SS1.p3.m2" class="ltx_Math" alttext="\{h_{n},\gamma_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{h_{n},\gamma_{n}\}_{n=1}^{N}</annotation></semantics></math>. In the context of machine learning this step is frequently referred to as <em class="ltx_emph ltx_font_italic">training</em>, and the labeled data <math id="SS1.p3.m3" class="ltx_Math" alttext="\{h_{n},\gamma_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{h_{n},\gamma_{n}\}_{n=1}^{N}</annotation></semantics></math> is referred to as the <em class="ltx_emph ltx_font_italic">training data</em>. Once <math id="SS1.p3.m4" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> is determined, we can use the learned parameterization <math id="SS1.p3.m5" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> on an unlabeled feature vector <math id="SS1.p3.m6" class="ltx_Math" alttext="h^{\mathrm{test}}" display="inline"><semantics><msup><mi>h</mi><mi>test</mi></msup><annotation encoding="application/x-tex">h^{\mathrm{test}}</annotation></semantics></math>, to compute an approximate MAP estimate for its label:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx6" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E6.m1" class="ltx_Math" alttext="\displaystyle{\gamma^{\mathrm{test}}}^{\star}\triangleq\arg\max_{{\gamma}\in%
\Gamma}f_{\boldsymbol{\gamma}|\boldsymbol{h},\boldsymbol{w}}(\gamma|h^{\mathrm%
{test}},w^{\star})" display="inline"><semantics><mrow><mmultiscripts><mi>γ</mi><mrow></mrow><mi>test</mi><mrow></mrow><mo>⋆</mo></mmultiscripts><mo>≜</mo><mrow><mrow><mi>arg</mi><mo lspace="0.167em">⁡</mo><mrow><munder><mi>max</mi><mrow><mi>γ</mi><mo>∈</mo><mi mathvariant="normal">Γ</mi></mrow></munder><mo lspace="0.167em">⁡</mo><msub><mi>f</mi><mrow><mi>𝜸</mi><mo fence="false">|</mo><mrow><mi>𝒉</mi><mo>,</mo><mi>𝒘</mi></mrow></mrow></msub></mrow></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>γ</mi><mo fence="false">|</mo><mrow><msup><mi>h</mi><mi>test</mi></msup><mo>,</mo><msup><mi>w</mi><mo>⋆</mo></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle{\gamma^{\mathrm{test}}}^{\star}\triangleq\arg\max_{{\gamma}\in%
\Gamma}f_{\boldsymbol{\gamma}|\boldsymbol{h},\boldsymbol{w}}(\gamma|h^{\mathrm%
{test}},w^{\star})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">We emphasize that (<a href="#E6" title="In 1.1. From Inference to Learning ‣ 1. BAYESIAN INFERENCE ‣ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) is only an <em class="ltx_emph ltx_font_italic">approximate</em> MAP estimate for the true label <math id="SS1.p3.m7" class="ltx_Math" alttext="{\gamma^{\mathrm{test}}}" display="inline"><semantics><msup><mi>γ</mi><mi>test</mi></msup><annotation encoding="application/x-tex">{\gamma^{\mathrm{test}}}</annotation></semantics></math>, since it is generated using an estimate of the posterior distribution <math id="SS1.p3.m8" class="ltx_Math" alttext="f_{\boldsymbol{\gamma}|\boldsymbol{h},\boldsymbol{w}}(\gamma|h^{\mathrm{test}}%
,w^{\star})" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>𝜸</mi><mo fence="false">|</mo><mrow><mi>𝒉</mi><mo>,</mo><mi>𝒘</mi></mrow></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>γ</mi><mo fence="false">|</mo><mrow><msup><mi>h</mi><mi>test</mi></msup><mo>,</mo><msup><mi>w</mi><mo>⋆</mo></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{\gamma}|\boldsymbol{h},\boldsymbol{w}}(\gamma|h^{\mathrm{test}}%
,w^{\star})</annotation></semantics></math> using the learned parameterization <math id="SS1.p3.m9" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math>. In general, and in particular for finite sample sizes <math id="SS1.p3.m10" class="ltx_Math" alttext="N" display="inline"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> of the training data, the learned parameterization <math id="SS1.p3.m11" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> will be different from the true parameterization <math id="SS1.p3.m12" class="ltx_Math" alttext="w^{o}" display="inline"><semantics><msup><mi>w</mi><mi>o</mi></msup><annotation encoding="application/x-tex">w^{o}</annotation></semantics></math> that actually generated the data. The difference between predictions made using the true model <math id="SS1.p3.m13" class="ltx_Math" alttext="w^{o}" display="inline"><semantics><msup><mi>w</mi><mi>o</mi></msup><annotation encoding="application/x-tex">w^{o}</annotation></semantics></math> and the learned model <math id="SS1.p3.m14" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> is known as a <em class="ltx_emph ltx_font_italic">generalization error</em>.</p>
</div>
<div id="Thmremark3" class="ltx_theorem ltx_theorem_remark">

<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem header-section-number"><span class="ltx_text ltx_font_italic">Remark 1.3</span></span><span class="ltx_text ltx_font_italic"> </span>(<span class="ltx_text ltx_font_bold">Compact notation for feature-label pairs</span>)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div id="Thmremark3.p1" class="ltx_para">
<p class="ltx_p">As is evident from (<a href="#E5" title="In 1.1. From Inference to Learning ‣ 1. BAYESIAN INFERENCE ‣ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), during learning, we are provided with feature-label pairs <math id="Thmremark3.p1.m1" class="ltx_Math" alttext="\{\boldsymbol{h},\boldsymbol{\gamma}\}" display="inline"><semantics><mrow><mo stretchy="false">{</mo><mi>𝒉</mi><mo>,</mo><mi>𝜸</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\boldsymbol{h},\boldsymbol{\gamma}\}</annotation></semantics></math> or their realizations <math id="Thmremark3.p1.m2" class="ltx_Math" alttext="\{h_{n},\gamma_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{h_{n},\gamma_{n}\}_{n=1}^{N}</annotation></semantics></math>. To simplify the notation, we will collect features <math id="Thmremark3.p1.m3" class="ltx_Math" alttext="\boldsymbol{h}\in\mathds{R}^{M_{\boldsymbol{h}}}" display="inline"><semantics><mrow><mi>𝒉</mi><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>M</mi><mi>𝒉</mi></msub></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{h}\in\mathds{R}^{M_{\boldsymbol{h}}}</annotation></semantics></math> and labels <math id="Thmremark3.p1.m4" class="ltx_Math" alttext="\boldsymbol{\gamma}\in\mathds{R}^{M_{\boldsymbol{\gamma}}}" display="inline"><semantics><mrow><mi>𝜸</mi><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>M</mi><mi>𝜸</mi></msub></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{\gamma}\in\mathds{R}^{M_{\boldsymbol{\gamma}}}</annotation></semantics></math> into a single augmented data vector <math id="Thmremark3.p1.m5" class="ltx_Math" alttext="{\boldsymbol{x}}\in\mathds{R}^{M_{\boldsymbol{h}}+M_{\boldsymbol{\gamma}}}" display="inline"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>M</mi><mi>𝒉</mi></msub><mo>+</mo><msub><mi>M</mi><mi>𝜸</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">{\boldsymbol{x}}\in\mathds{R}^{M_{\boldsymbol{h}}+M_{\boldsymbol{\gamma}}}</annotation></semantics></math>, such that:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx7" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(7)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E7.m1" class="ltx_Math" alttext="\displaystyle{\boldsymbol{x}}\triangleq\mathrm{col}\left\{\boldsymbol{h},%
\boldsymbol{\gamma}\right\}=\begin{pmatrix}\boldsymbol{h}\\
\boldsymbol{\gamma}\end{pmatrix}" display="inline"><semantics><mrow><mi>𝒙</mi><mo>≜</mo><mrow><mi>col</mi><mo>⁢</mo><mrow><mo>{</mo><mi>𝒉</mi><mo>,</mo><mi>𝜸</mi><mo>}</mo></mrow></mrow><mo>=</mo><mrow><mo>(</mo><mtable rowspacing="0pt"><mtr><mtd><mi>𝒉</mi></mtd></mtr><mtr><mtd><mi>𝜸</mi></mtd></mtr></mtable><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle{\boldsymbol{x}}\triangleq\mathrm{col}\left\{\boldsymbol{h},%
\boldsymbol{\gamma}\right\}=\begin{pmatrix}\boldsymbol{h}\\
\boldsymbol{\gamma}\end{pmatrix}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">Similarly, we will collect realizations into <math id="Thmremark3.p1.m6" class="ltx_Math" alttext="x_{n}\triangleq\mathrm{col}\left\{h_{n},\gamma_{n}\right\}" display="inline"><semantics><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>≜</mo><mrow><mi>col</mi><mo>⁢</mo><mrow><mo>{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>n</mi></msub><mo>}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">x_{n}\triangleq\mathrm{col}\left\{h_{n},\gamma_{n}\right\}</annotation></semantics></math>. In this manner, we can write more compactly:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx8" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(8)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E8.m1" class="ltx_Math" alttext="\displaystyle f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left({h},{%
\gamma}|w\right)=f_{{\boldsymbol{x}}|\boldsymbol{w}}\left(x|w\right)" display="inline"><semantics><mrow><mrow><msub><mi>f</mi><mrow><mi>𝒉</mi><mo>,</mo><mrow><mi>𝜸</mi><mo fence="false">|</mo><mi>𝒘</mi></mrow></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mi>h</mi><mo>,</mo><mrow><mi>γ</mi><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>f</mi><mrow><mi>𝒙</mi><mo fence="false">|</mo><mi>𝒘</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left({h},{%
\gamma}|w\right)=f_{{\boldsymbol{x}}|\boldsymbol{w}}\left(x|w\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">The MAP learning problem (<a href="#E5" title="In 1.1. From Inference to Learning ‣ 1. BAYESIAN INFERENCE ‣ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) then becomes:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx9" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(9)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E9.m1" class="ltx_Math" alttext="\displaystyle w^{\star}=" display="inline"><semantics><mrow><msup><mi>w</mi><mo>⋆</mo></msup><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle w^{\star}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="E9.m2" class="ltx_Math" alttext="\displaystyle\&gt;\arg\min_{w\in\mathds{R}^{M}}\left\{-\frac{1}{N}\sum_{n=1}^{N}%
\log f_{{\boldsymbol{x}}|\boldsymbol{w}}\left(x_{n}|w\right)-\frac{1}{N}\log f%
_{\boldsymbol{w}}(w)\right\}" display="inline"><semantics><mrow><mi>arg</mi><mo lspace="0.167em">⁡</mo><mrow><munder><mi>min</mi><mrow><mi>w</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>M</mi></msup></mrow></munder><mo>⁡</mo><mrow><mo>{</mo><mrow><mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>f</mi><mrow><mi>𝒙</mi><mo fence="false">|</mo><mi>𝒘</mi></mrow></msub></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>n</mi></msub><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo lspace="0.167em">⁢</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>f</mi><mi>𝒘</mi></msub></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\arg\min_{w\in\mathds{R}^{M}}\left\{-\frac{1}{N}\sum_{n=1}^{N}%
\log f_{{\boldsymbol{x}}|\boldsymbol{w}}\left(x_{n}|w\right)-\frac{1}{N}\log f%
_{\boldsymbol{w}}(w)\right\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
</div>
</div>
</section>
</div>
</section>
</div>
</div>
</div>
<a href="index.html" class="navigation navigation-prev" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
</div>
</div>
<script src="bookml/gitbook/js/app.min.js"></script>
<script src="bookml/gitbook/js/plugin-fontsettings.js"></script>
<script src="bookml/gitbook/js/plugin-bookdown.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
        MathJax = {
          startup: {
            ready() {
              // do not process equations disabled with \bmlDisableMathJax (code suggested by Davide P. Cervone)
              class bmlFindMathML extends MathJax._.input.mathml.FindMathML.FindMathML {
                processMath(set) {
                  const adaptor = this.adaptor;
                  for (const node of set.values()) {
                    if (adaptor.hasClass(node, 'bml_disable_mathjax')) {
                      set.delete(node);
                    }
                  }
                  return super.processMath(set);
                }
              }

              MathJax._.components.global.combineDefaults(MathJax.config, 'mml', {FindMathML: new bmlFindMathML()});

              MathJax.startup.defaultReady();

              // preproces MathML to make MathJax aware of certain LaTeXML and BookML additional info
              const mmlFilters = MathJax.startup.input[0].mmlFilters;

              // convert the LaTeXML calligraphic (chancery) annotation to a form MathJax understands
              // since the corresponding Unicode characters render as script (rounded)
              const script2latin = {
                '𝒜': 'A', 'ℬ': 'B', '𝒞': 'C', '𝒟': 'D', 'ℰ': 'E', 'ℱ': 'F', '𝒢': 'G',
                'ℋ': 'H', 'ℐ': 'I', '𝒥': 'J', '𝒦': 'K', 'ℒ': 'L', 'ℳ': 'M', '𝒩': 'N',
                '𝒪': 'O', '𝒫': 'P', '𝒬': 'Q', 'ℛ': 'R', '𝒮': 'S', '𝒯': 'T', '𝒰': 'U',
                '𝒱': 'V', '𝒲': 'W', '𝒳': 'X', '𝒴': 'Y', '𝒵': 'Z',
              };

              mmlFilters.add((args) => {
                for (const n of args.data.getElementsByClassName('ltx_font_mathcaligraphic')) {
                  n.classList.add('MJX-tex-calligraphic');
                  const letter = script2latin[n.textContent];
                  if (letter !== undefined) { n.textContent = letter; }
                }
              });

              // adjust characters based on Unicode variation sequences
              const replacements = {
                // MathJax renders the empty set as the U+FE00 variant, so the plain character needs adjusting
                '∅': { variant: 'variant' },
                // MathJax renders script characters in rounded style, which is fine for no variation and U+FE00
                '𝒜\xFE00': { text: 'A', variant: 'tex-calligraphic' },
                'ℬ\xFE00': { text: 'B', variant: 'tex-calligraphic' },
                '𝒞\xFE00': { text: 'C', variant: 'tex-calligraphic' },
                '𝒟\xFE00': { text: 'D', variant: 'tex-calligraphic' },
                'ℰ\xFE00': { text: 'E', variant: 'tex-calligraphic' },
                'ℱ\xFE00': { text: 'F', variant: 'tex-calligraphic' },
                '𝒢\xFE00': { text: 'G', variant: 'tex-calligraphic' },
                'ℋ\xFE00': { text: 'H', variant: 'tex-calligraphic' },
                'ℐ\xFE00': { text: 'I', variant: 'tex-calligraphic' },
                '𝒥\xFE00': { text: 'J', variant: 'tex-calligraphic' },
                '𝒦\xFE00': { text: 'K', variant: 'tex-calligraphic' },
                'ℒ\xFE00': { text: 'L', variant: 'tex-calligraphic' },
                'ℳ\xFE00': { text: 'M', variant: 'tex-calligraphic' },
                '𝒩\xFE00': { text: 'N', variant: 'tex-calligraphic' },
                '𝒪\xFE00': { text: 'O', variant: 'tex-calligraphic' },
                '𝒫\xFE00': { text: 'P', variant: 'tex-calligraphic' },
                '𝒬\xFE00': { text: 'Q', variant: 'tex-calligraphic' },
                'ℛ\xFE00': { text: 'R', variant: 'tex-calligraphic' },
                '𝒮\xFE00': { text: 'S', variant: 'tex-calligraphic' },
                '𝒯\xFE00': { text: 'T', variant: 'tex-calligraphic' },
                '𝒰\xFE00': { text: 'U', variant: 'tex-calligraphic' },
                '𝒱\xFE00': { text: 'V', variant: 'tex-calligraphic' },
                '𝒲\xFE00': { text: 'W', variant: 'tex-calligraphic' },
                '𝒳\xFE00': { text: 'X', variant: 'tex-calligraphic' },
                '𝒴\xFE00': { text: 'Y', variant: 'tex-calligraphic' },
                '𝒵\xFE00': { text: 'Z', variant: 'tex-calligraphic' }
              };

              mmlFilters.add((args) => {
                let nodes = document.evaluate('.//m:mi | .//m:mn | .//m:mo | .//m:ms', args.data,
                  () => 'http://www.w3.org/1998/Math/MathML', XPathResult.UNORDERED_NODE_SNAPSHOT_TYPE);
                for (let i = 0; i < nodes.snapshotLength; i++) {
                  const n = nodes.snapshotItem(i);
                  const repl = replacements[n.innerHTML];
                  if (repl !== undefined) {
                    const variant = repl['variant'];
                    const text = repl['text'];
                    if (variant !== undefined) { n.classList.add('MJX-' + variant); n.removeAttribute('mathvariant'); }
                    if (text !== undefined) { n.innerHTML = text; }
                  }
                }
              });
            }
          }
        };
      </script>
<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js"></script>
<script type="text/javascript">
              gitbook.require(["gitbook"], function(gitbook) {
              gitbook.start({
                "fontsettings": {
                  "theme": "white",
                  "family": "sans",
                  "size": 2
                },
                "download": [ [ "lecture1.pdf", "PDF (serif)" ], ],
                  "search": false,
                  "toc": {
                    "collapse": "none"
                  }
                });
              });
            </script>
</body>

</html>
