<!DOCTYPE html><html lang="en-GB">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>1. BAYESIAN INFERENCE â€£ Lecture 1: Foundations in Deterministic and Stochastic Optimization</title>
<!--Generated on Mon Mar 18 14:03:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<link rel="stylesheet" href="ltx-amsart.css" type="text/css">
<link rel="stylesheet" href="bookml/gitbook/css/style.css" type="text/css">
<link rel="stylesheet" href="bookml/gitbook/css/plugin-table.css" type="text/css">
<link rel="stylesheet" href="bookml/gitbook/css/plugin-bookdown.css" type="text/css">
<link rel="stylesheet" href="bookml/gitbook/css/plugin-fontsettings.css" type="text/css">
<link rel="stylesheet" href="bookml/CSS/style.css" type="text/css">
<link rel="stylesheet" href="bookml/CSS/style.gitbook,plain.css" type="text/css">
<link rel="stylesheet" href="bookml/CSS/style.gitbook.css" type="text/css">
<link rel="stylesheet" href="bmluser/lecture1.colors.gitbook.css" type="text/css">
<link rel="stylesheet" href="bmluser/template.colors.gitbook.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<script src="https://code.jquery.com/jquery-3.6.0.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<link rel="up" href="index.html" title="Lecture 1: Foundations in Deterministic and Stochastic Optimization">
<link rel="start" href="index.html" title="Lecture 1: Foundations in Deterministic and Stochastic Optimization">
<link rel="prev" href="index.html" title="Lecture 1: Foundations in Deterministic and Stochastic Optimization">
</head>
<body>
<div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">
<a href="#bml-main-content" tabindex="0" class="bml-skip-to-content">Skip to content.</a>
<div class="book-summary"><nav class="ltx_page_navbar">
<ul class="ltx_toclist summary">
<li>
<a href="index.html" title="" class="ltx_ref" rel="start">Lecture 1: Foundations in Deterministic and Stochastic Optimization</a>
</li>
<li class="divider">
<li data-level="1" data-path="" class="ltx_tocentry ltx_tocentry_section ltx_ref_self  chapter active">
<a href="" title="In Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><b><span class="ltx_tag ltx_tag_ref">1 </span></b>Bayesian Inference</a>
<ol class="ltx_toclist ltx_toclist_section">
<li data-level="1.1" data-path="" class="ltx_tocentry ltx_tocentry_subsection  chapter "><a href="#SS1" title="In 1. BAYESIAN INFERENCE â€£ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><b><span class="ltx_tag ltx_tag_ref">1.1 </span></b>From Inference to Learning</a></li>
</ol>
</li>
</ul>
</nav></div>
<div class="ltx_page_main book-body fixed">
<div class="body-inner">
<div class="book-header fixed" role="navigation"><h1>BAYESIAN INFERENCE</h1></div>
<div class="page-wrapper" tabindex="-1" role="main">
<div class="ltx_page_content page-inner" id="bml-main-content">
<section class="ltx_section ltx_authors_1line ltx_leqno normal" lang="en-GB">
<div class="section level1">

<h1 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section header-section-number">1</span>. BAYESIAN INFERENCE</h1>

<div id="p1" class="ltx_para">
<p class="ltx_p">One of the most fundamental problems in statistics, signal processing, and machine learning, is the <em class="ltx_emph ltx_font_italic">inference</em> problem, where we wish to construct an estimate of some random quantity of interest <math id="p1.m1" class="ltx_Math" alttext="\boldsymbol{\gamma}\in\mathds{R}^{M_{\boldsymbol{\gamma}}}" display="inline"><semantics><mrow><mi>ğœ¸</mi><mo>âˆˆ</mo><msup><mi>â„</mi><msub><mi>M</mi><mi>ğœ¸</mi></msub></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{\gamma}\in\mathds{R}^{M_{\boldsymbol{\gamma}}}</annotation></semantics></math>, given observations of a related random variable <math id="p1.m2" class="ltx_Math" alttext="\boldsymbol{h}\in\mathds{R}^{M_{\boldsymbol{h}}}" display="inline"><semantics><mrow><mi>ğ’‰</mi><mo>âˆˆ</mo><msup><mi>â„</mi><msub><mi>M</mi><mi>ğ’‰</mi></msub></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{h}\in\mathds{R}^{M_{\boldsymbol{h}}}</annotation></semantics></math>. The quantity of interest <math id="p1.m3" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>ğœ¸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math> may take on continuous or discrete values, and is referred to as the â€œdependent variable,â€ â€œstate of nature,â€ â€œclass,â€ or â€œlabelâ€ depending on the application. We will most commonly refer to it as the label. We will refer to the observed random variable <math id="p1.m4" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>ğ’‰</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math> generally as the feature, although in some applications it is known as â€œregressorâ€ or â€œobservationâ€.</p>
</div>
<div id="p2" class="ltx_para">
<p class="ltx_p">If we are provided with the conditional distribution <math id="p2.m1" class="ltx_Math" alttext="f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|h)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>ğœ¸</mi><mo fence="false">|</mo><mi>ğ’‰</mi></mrow></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Î³</mi><mo fence="false">|</mo><mi>h</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|h)</annotation></semantics></math> along with a single realization of the feature <math id="p2.m2" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>ğ’‰</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>, it is quite natural to estimate <math id="p2.m3" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>ğœ¸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math> as the most likely outcome given <math id="p2.m4" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>ğ’‰</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>. We can express this formally as:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E1.m1" class="ltx_Math" alttext="\displaystyle{\gamma^{\star}}\triangleq\arg\max_{{\gamma}\in\Gamma}f_{%
\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|h)" display="inline"><semantics><mrow><msup><mi>Î³</mi><mo>â‹†</mo></msup><mo>â‰œ</mo><mrow><mrow><mi>arg</mi><mo lspace="0.167em">â¡</mo><mrow><munder><mi>max</mi><mrow><mi>Î³</mi><mo>âˆˆ</mo><mi mathvariant="normal">Î“</mi></mrow></munder><mo lspace="0.167em">â¡</mo><msub><mi>f</mi><mrow><mi>ğœ¸</mi><mo fence="false">|</mo><mi>ğ’‰</mi></mrow></msub></mrow></mrow><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Î³</mi><mo fence="false">|</mo><mi>h</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle{\gamma^{\star}}\triangleq\arg\max_{{\gamma}\in\Gamma}f_{%
\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|h)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">The conditional distribution <math id="p2.m5" class="ltx_Math" alttext="f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|h)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>ğœ¸</mi><mo fence="false">|</mo><mi>ğ’‰</mi></mrow></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Î³</mi><mo fence="false">|</mo><mi>h</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|h)</annotation></semantics></math> is frequently referred to as the <em class="ltx_emph ltx_font_italic">posterior distribution</em> of <math id="p2.m6" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>ğœ¸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math>, making <math id="p2.m7" class="ltx_Math" alttext="{\gamma^{\star}}" display="inline"><semantics><msup><mi>Î³</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">{\gamma^{\star}}</annotation></semantics></math> the <em class="ltx_emph ltx_font_italic">maximum a posteriori (MAP) estimate</em> of <math id="p2.m8" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>ğœ¸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math> given <math id="p2.m9" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>ğ’‰</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>. We note that it is common in the literature on estimation theory to the employ hat-notation to refer to estimates of a random quantity based on data. In this sense, we could have opted to denote the optimal solution toÂ (<a href="#E1" title="In 1. BAYESIAN INFERENCE â€£ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) by <math id="p2.m10" class="ltx_Math" alttext="\widehat{\gamma}" display="inline"><semantics><mover accent="true"><mi>Î³</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\widehat{\gamma}</annotation></semantics></math>, rather than <math id="p2.m11" class="ltx_Math" alttext="\gamma^{\star}" display="inline"><semantics><msup><mi>Î³</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">\gamma^{\star}</annotation></semantics></math>. Nevertheless, as we transition from inference to learning and optimization, we will increasingly interpret estimates as solutions to generic optimization problems. In this context, it will be appropriate to employ the <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">â‹†</span></sup>-notation to refer to an optimal solution. To preserve consistency throughout this text, we opt to use the <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">â‹†</span></sup>-notation from the onset, with the understanding that within the Bayesian frameworkÂ (<a href="#E1" title="In 1. BAYESIAN INFERENCE â€£ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) the optimal solution <math id="p2.m14" class="ltx_Math" alttext="\gamma^{\star}" display="inline"><semantics><msup><mi>Î³</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">\gamma^{\star}</annotation></semantics></math> carries the interpretation of an estimate. Finally, we note thatÂ (<a href="#E1" title="In 1. BAYESIAN INFERENCE â€£ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) is formulated for a particular realization <math id="p2.m15" class="ltx_Math" alttext="h" display="inline"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math> of the random variable <math id="p2.m16" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>ğ’‰</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>. As we will see, most MAP estimators are derived for a particular realization of the feature vector, resulting in a deterministic estimate <math id="p2.m17" class="ltx_Math" alttext="\gamma^{\star}" display="inline"><semantics><msup><mi>Î³</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">\gamma^{\star}</annotation></semantics></math> as a function of the realization <math id="p2.m18" class="ltx_Math" alttext="h" display="inline"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>. We can also regard the MAP solution as a random variable, denoted in boldface notation <math id="p2.m19" class="ltx_Math" alttext="\boldsymbol{\gamma}^{\star}" display="inline"><semantics><msup><mi>ğœ¸</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">\boldsymbol{\gamma}^{\star}</annotation></semantics></math>, when it is viewed as a function of the random observation <math id="p2.m20" class="ltx_Math" alttext="{\boldsymbol{h}}" display="inline"><semantics><mi>ğ’‰</mi><annotation encoding="application/x-tex">{\boldsymbol{h}}</annotation></semantics></math>:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E2.m1" class="ltx_Math" alttext="\displaystyle\boldsymbol{\gamma}^{\star}\triangleq\arg\max_{{\gamma}\in\Gamma}%
f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|\boldsymbol{h})" display="inline"><semantics><mrow><msup><mi>ğœ¸</mi><mo>â‹†</mo></msup><mo>â‰œ</mo><mrow><mrow><mi>arg</mi><mo lspace="0.167em">â¡</mo><mrow><munder><mi>max</mi><mrow><mi>Î³</mi><mo>âˆˆ</mo><mi mathvariant="normal">Î“</mi></mrow></munder><mo lspace="0.167em">â¡</mo><msub><mi>f</mi><mrow><mi>ğœ¸</mi><mo fence="false">|</mo><mi>ğ’‰</mi></mrow></msub></mrow></mrow><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Î³</mi><mo fence="false">|</mo><mi>ğ’‰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\boldsymbol{\gamma}^{\star}\triangleq\arg\max_{{\gamma}\in\Gamma}%
f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|\boldsymbol{h})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">where the posterior distribution <math id="p2.m21" class="ltx_Math" alttext="f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|\boldsymbol{h})" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>ğœ¸</mi><mo fence="false">|</mo><mi>ğ’‰</mi></mrow></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Î³</mi><mo fence="false">|</mo><mi>ğ’‰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{\gamma}|\boldsymbol{h}}(\gamma|\boldsymbol{h})</annotation></semantics></math> is now a function of the random variable <math id="p2.m22" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>ğ’‰</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>, rather than its realization <math id="p2.m23" class="ltx_Math" alttext="h" display="inline"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>.</p>
</div>
<div id="Thmremark1" class="ltx_theorem ltx_theorem_remark">

<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem header-section-number"><span class="ltx_text ltx_font_italic">Remark 1.1</span></span><span class="ltx_text ltx_font_italic"> </span>(<span class="ltx_text ltx_font_bold"> Bold font indicates random variables</span>)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div id="Thmremark1.p1" class="ltx_para">
<p class="ltx_p">We note that we employ bold font to denote a random variables, for example <math id="Thmremark1.p1.m1" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>ğ’‰</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>, while regular font denotes its realization or a deterministic quanity, as in <math id="Thmremark1.p1.m2" class="ltx_Math" alttext="h" display="inline"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>. As a general rule, we use lowercase fonts to denote vectors, while uppercase letters denote matrices. In this way, a random matrix would be denoted by <math id="Thmremark1.p1.m3" class="ltx_Math" alttext="\boldsymbol{H}" display="inline"><semantics><mi>ğ‘¯</mi><annotation encoding="application/x-tex">\boldsymbol{H}</annotation></semantics></math>, while its realization would correspond to <math id="Thmremark1.p1.m4" class="ltx_Math" alttext="H" display="inline"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>.</p>
</div>
</div>
<figure id="F1" class="ltx_figure"><div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;"><img src="" id="F1.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption"></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span><span class="ltx_text" style="font-size:90%;">A general model underlying most inference problems. The top row illustrates the generative model for the observations <math id="F1.m2" class="ltx_Math" alttext="{\boldsymbol{h}}" display="inline"><semantics><mi>ğ’‰</mi><annotation encoding="application/x-tex">{\boldsymbol{h}}</annotation></semantics></math>, while the bottom row illustrates the Bayesian inference formulation for recovering the label variable.</span></figcaption>
</figure>
<section id="SS1" class="ltx_subsection normal">
<div class="section level2">

<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection header-section-number">1.1</span>. From Inference to Learning</h2>

<div id="SS1.p1" class="ltx_para">
<p class="ltx_p">Our previous discussion leading has uncovered how to perform MAP inference of a random quantity of interest <math id="SS1.p1.m1" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>ğœ¸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math> given observations <math id="SS1.p1.m2" class="ltx_Math" alttext="\{\boldsymbol{h}_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>ğ’‰</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n}\}_{n=1}^{N}</annotation></semantics></math>, models <math id="SS1.p1.m3" class="ltx_math_unparsed" alttext="f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>ğ’‰</mi><mo fence="false">|</mo><mi>ğœ¸</mi></mrow></msub><mrow><mo>(</mo><mo lspace="0em" rspace="0em">â‹…</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>Î³</mi><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)</annotation></semantics></math>, and prior information <math id="SS1.p1.m4" class="ltx_Math" alttext="f_{\boldsymbol{\gamma}}(\gamma)" display="inline"><semantics><mrow><msub><mi>f</mi><mi>ğœ¸</mi></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>Î³</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{\gamma}}(\gamma)</annotation></semantics></math>. In many practical situations, we are not provided with prior knowledge about the model relating the label <math id="SS1.p1.m5" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>ğœ¸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math> with the feature <math id="SS1.p1.m6" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>ğ’‰</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math>. Instead, we need to estimate <math id="SS1.p1.m7" class="ltx_math_unparsed" alttext="f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>ğ’‰</mi><mo fence="false">|</mo><mi>ğœ¸</mi></mrow></msub><mrow><mo>(</mo><mo lspace="0em" rspace="0em">â‹…</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>Î³</mi><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)</annotation></semantics></math> from data before performing inference. We refer to the process of estimating statistical properties of data needed for inference, such as <math id="SS1.p1.m8" class="ltx_math_unparsed" alttext="f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>ğ’‰</mi><mo fence="false">|</mo><mi>ğœ¸</mi></mrow></msub><mrow><mo>(</mo><mo lspace="0em" rspace="0em">â‹…</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>Î³</mi><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)</annotation></semantics></math>, as <em class="ltx_emph ltx_font_italic">learning</em>. As we will see in this section, the learning of models can be formalized using a Bayesian MAP framework analogous toÂ (<a href="#E1" title="In 1. BAYESIAN INFERENCE â€£ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). To this end, we will assume that the conditional likelihood <math id="SS1.p1.m9" class="ltx_math_unparsed" alttext="f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>ğ’‰</mi><mo fence="false">|</mo><mi>ğœ¸</mi></mrow></msub><mrow><mo>(</mo><mo lspace="0em" rspace="0em">â‹…</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>Î³</mi><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h}|\boldsymbol{\gamma}}\left(\cdot|{\gamma}\right)</annotation></semantics></math> is parameterized by a learnable parameter <math id="SS1.p1.m10" class="ltx_Math" alttext="w\in\mathds{R}^{M_{w}}" display="inline"><semantics><mrow><mi>w</mi><mo>âˆˆ</mo><msup><mi>â„</mi><msub><mi>M</mi><mi>w</mi></msub></msup></mrow><annotation encoding="application/x-tex">w\in\mathds{R}^{M_{w}}</annotation></semantics></math> and write instead <math id="SS1.p1.m11" class="ltx_math_unparsed" alttext="f_{\boldsymbol{h}|\boldsymbol{\gamma},\boldsymbol{w}}\left(\cdot|{\gamma},w\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>ğ’‰</mi><mo fence="false">|</mo><mrow><mi>ğœ¸</mi><mo>,</mo><mi>ğ’˜</mi></mrow></mrow></msub><mrow><mo>(</mo><mo lspace="0em" rspace="0em">â‹…</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>Î³</mi><mo>,</mo><mi>w</mi><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h}|\boldsymbol{\gamma},\boldsymbol{w}}\left(\cdot|{\gamma},w\right)</annotation></semantics></math>. Under this parameterization, learning the conditional distribution of <math id="SS1.p1.m12" class="ltx_Math" alttext="\boldsymbol{\gamma}" display="inline"><semantics><mi>ğœ¸</mi><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math> given <math id="SS1.p1.m13" class="ltx_Math" alttext="\boldsymbol{h}" display="inline"><semantics><mi>ğ’‰</mi><annotation encoding="application/x-tex">\boldsymbol{h}</annotation></semantics></math> is equivalent to learning the parameter <math id="SS1.p1.m14" class="ltx_Math" alttext="\boldsymbol{w}" display="inline"><semantics><mi>ğ’˜</mi><annotation encoding="application/x-tex">\boldsymbol{w}</annotation></semantics></math> that parameterizes <math id="SS1.p1.m15" class="ltx_math_unparsed" alttext="f_{\boldsymbol{h}|\boldsymbol{\gamma},\boldsymbol{w}}\left(\cdot|{\gamma},w\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>ğ’‰</mi><mo fence="false">|</mo><mrow><mi>ğœ¸</mi><mo>,</mo><mi>ğ’˜</mi></mrow></mrow></msub><mrow><mo>(</mo><mo lspace="0em" rspace="0em">â‹…</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>Î³</mi><mo>,</mo><mi>w</mi><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h}|\boldsymbol{\gamma},\boldsymbol{w}}\left(\cdot|{\gamma},w\right)</annotation></semantics></math>.</p>
</div>
<div id="SS1.p2" class="ltx_para">
<p class="ltx_p">To formulate a procedure for learning <math id="SS1.p2.m1" class="ltx_Math" alttext="\boldsymbol{w}" display="inline"><semantics><mi>ğ’˜</mi><annotation encoding="application/x-tex">\boldsymbol{w}</annotation></semantics></math> from pairs <math id="SS1.p2.m2" class="ltx_Math" alttext="\{\boldsymbol{h},\boldsymbol{\gamma}\}" display="inline"><semantics><mrow><mo stretchy="false">{</mo><mi>ğ’‰</mi><mo>,</mo><mi>ğœ¸</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\boldsymbol{h},\boldsymbol{\gamma}\}</annotation></semantics></math> we mirror the argument in SectionÂ <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:sec:estimates_using_a_batch_of_samples</span>. Suppose the model <math id="SS1.p2.m3" class="ltx_Math" alttext="{\boldsymbol{w}}" display="inline"><semantics><mi>ğ’˜</mi><annotation encoding="application/x-tex">{\boldsymbol{w}}</annotation></semantics></math> is sampled once, yielding the realization <math id="SS1.p2.m4" class="ltx_Math" alttext="w^{o}" display="inline"><semantics><msup><mi>w</mi><mi>o</mi></msup><annotation encoding="application/x-tex">w^{o}</annotation></semantics></math>. The <em class="ltx_emph ltx_font_italic">training data</em> <math id="SS1.p2.m5" class="ltx_Math" alttext="\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>ğ’‰</mi><mi>n</mi></msub><mo>,</mo><msub><mi>ğœ¸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}</annotation></semantics></math> is sampled <math id="SS1.p2.m6" class="ltx_Math" alttext="N" display="inline"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> times, where each pair <math id="SS1.p2.m7" class="ltx_Math" alttext="\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}" display="inline"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>ğ’‰</mi><mi>n</mi></msub><mo>,</mo><msub><mi>ğœ¸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}</annotation></semantics></math> is sampled from <math id="SS1.p2.m8" class="ltx_Math" alttext="f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left(h,{\gamma}|w^{o}\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>ğ’‰</mi><mo>,</mo><mrow><mi>ğœ¸</mi><mo fence="false">|</mo><mi>ğ’˜</mi></mrow></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><mi>h</mi><mo>,</mo><mrow><mi>Î³</mi><mo fence="false">|</mo><msup><mi>w</mi><mi>o</mi></msup></mrow><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left(h,{\gamma}|w^{o}\right)</annotation></semantics></math>. If we suppose that feature-label pairs <math id="SS1.p2.m9" class="ltx_Math" alttext="\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}" display="inline"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>ğ’‰</mi><mi>n</mi></msub><mo>,</mo><msub><mi>ğœ¸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}</annotation></semantics></math> are identically and independently distributed after conditioning on the parameterization <math id="SS1.p2.m10" class="ltx_Math" alttext="{\boldsymbol{w}}" display="inline"><semantics><mi>ğ’˜</mi><annotation encoding="application/x-tex">{\boldsymbol{w}}</annotation></semantics></math>, we can factorize:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="Ex1.m1" class="ltx_Math" alttext="\displaystyle f_{\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}|%
\boldsymbol{w}}\left(\left\{{h}_{n},\gamma_{n}\right\}_{n=1}^{N}|w\right)%
\stackrel{{\scriptstyle(a)}}{{=}}" display="inline"><semantics><mrow><mrow><msub><mi>f</mi><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>ğ’‰</mi><mi>n</mi></msub><mo>,</mo><msub><mi>ğœ¸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo fence="false">|</mo><mi>ğ’˜</mi></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><msubsup><mrow><mo>{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>Î³</mi><mi>n</mi></msub><mo>}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow><mover><mo>=</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mover><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle f_{\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}|%
\boldsymbol{w}}\left(\left\{{h}_{n},\gamma_{n}\right\}_{n=1}^{N}|w\right)%
\stackrel{{\scriptstyle(a)}}{{=}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="Ex1.m2" class="ltx_Math" alttext="\displaystyle\&gt;\prod_{n=1}^{N}f_{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}|%
\boldsymbol{w}}\left({h}_{n},{\gamma}_{n}|w\right)" display="inline"><semantics><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">âˆ</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>f</mi><mrow><msub><mi>ğ’‰</mi><mi>n</mi></msub><mo>,</mo><mrow><msub><mi>ğœ¸</mi><mi>n</mi></msub><mo fence="false">|</mo><mi>ğ’˜</mi></mrow></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><mrow><msub><mi>Î³</mi><mi>n</mi></msub><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\prod_{n=1}^{N}f_{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}|%
\boldsymbol{w}}\left({h}_{n},{\gamma}_{n}|w\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E3.m1" class="ltx_Math" alttext="\displaystyle\stackrel{{\scriptstyle(b)}}{{=}}" display="inline"><semantics><mover><mo>=</mo><mrow><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mover><annotation encoding="application/x-tex">\displaystyle\stackrel{{\scriptstyle(b)}}{{=}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="E3.m2" class="ltx_Math" alttext="\displaystyle\&gt;\prod_{n=1}^{N}f_{\boldsymbol{h},\boldsymbol{\gamma}|%
\boldsymbol{w}}\left({h}_{n},{\gamma}_{n}|w\right)" display="inline"><semantics><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">âˆ</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>f</mi><mrow><mi>ğ’‰</mi><mo>,</mo><mrow><mi>ğœ¸</mi><mo fence="false">|</mo><mi>ğ’˜</mi></mrow></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><mrow><msub><mi>Î³</mi><mi>n</mi></msub><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\prod_{n=1}^{N}f_{\boldsymbol{h},\boldsymbol{\gamma}|%
\boldsymbol{w}}\left({h}_{n},{\gamma}_{n}|w\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">Step <math id="SS1.p2.m11" class="ltx_Math" alttext="(a)" display="inline"><semantics><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(a)</annotation></semantics></math> holds by conditional independence and <math id="SS1.p2.m12" class="ltx_Math" alttext="(b)" display="inline"><semantics><mrow><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(b)</annotation></semantics></math> holds by identical distribution of the pairs of random variables <math id="SS1.p2.m13" class="ltx_Math" alttext="\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}" display="inline"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>ğ’‰</mi><mi>n</mi></msub><mo>,</mo><msub><mi>ğœ¸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}</annotation></semantics></math>. We can then define the MAP estimate of the weight vector <math id="SS1.p2.m14" class="ltx_Math" alttext="w" display="inline"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math> as:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="Ex2.m1" class="ltx_Math" alttext="\displaystyle w^{\star}\triangleq" display="inline"><semantics><mrow><msup><mi>w</mi><mo>â‹†</mo></msup><mo>â‰œ</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle w^{\star}\triangleq</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="Ex2.m2" class="ltx_Math" alttext="\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}f_{\boldsymbol{w}|\{%
\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}}\left(w|\left\{{h}_{n},%
\gamma_{n}\right\}_{n=1}^{N}\right)" display="inline"><semantics><mrow><mrow><mi>arg</mi><mo lspace="0.167em">â¡</mo><mrow><munder><mi>max</mi><mrow><mi>w</mi><mo>âˆˆ</mo><msup><mi>â„</mi><msub><mi>M</mi><mi>w</mi></msub></msup></mrow></munder><mo lspace="0.167em">â¡</mo><msub><mi>f</mi><mrow><mi>ğ’˜</mi><mo fence="false">|</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>ğ’‰</mi><mi>n</mi></msub><mo>,</mo><msub><mi>ğœ¸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow></msub></mrow></mrow><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo fence="false">|</mo><msubsup><mrow><mo>{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>Î³</mi><mi>n</mi></msub><mo>}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}f_{\boldsymbol{w}|\{%
\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}}\left(w|\left\{{h}_{n},%
\gamma_{n}\right\}_{n=1}^{N}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="Ex3.m1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="Ex3.m2" class="ltx_Math" alttext="\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}\frac{f_{\{\boldsymbol{h}_{n},%
\boldsymbol{\gamma}_{n}\}_{n=1}^{N}|\boldsymbol{w}}\left(\left\{{h}_{n},\gamma%
_{n}\right\}_{n=1}^{N}|w\right)\times f_{\boldsymbol{w}}(w)}{f_{\{\boldsymbol{%
h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}}\left(\left\{{h}_{n},\gamma_{n}%
\right\}_{n=1}^{N}\right)}" display="inline"><semantics><mrow><mi>arg</mi><mo lspace="0.167em">â¡</mo><mrow><munder><mi>max</mi><mrow><mi>w</mi><mo>âˆˆ</mo><msup><mi>â„</mi><msub><mi>M</mi><mi>w</mi></msub></msup></mrow></munder><mo lspace="0.167em">â¡</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><mrow><msub><mi>f</mi><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>ğ’‰</mi><mi>n</mi></msub><mo>,</mo><msub><mi>ğœ¸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo fence="false">|</mo><mi>ğ’˜</mi></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><msubsup><mrow><mo>{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>Î³</mi><mi>n</mi></msub><mo>}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo fence="false">|</mo><mi>w</mi></mrow><mo rspace="0.055em">)</mo></mrow></mrow><mo rspace="0.222em">Ã—</mo><msub><mi>f</mi><mi>ğ’˜</mi></msub></mrow><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>f</mi><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>ğ’‰</mi><mi>n</mi></msub><mo>,</mo><msub><mi>ğœ¸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></msub><mo>â¢</mo><mrow><mo>(</mo><msubsup><mrow><mo>{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>Î³</mi><mi>n</mi></msub><mo>}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}\frac{f_{\{\boldsymbol{h}_{n},%
\boldsymbol{\gamma}_{n}\}_{n=1}^{N}|\boldsymbol{w}}\left(\left\{{h}_{n},\gamma%
_{n}\right\}_{n=1}^{N}|w\right)\times f_{\boldsymbol{w}}(w)}{f_{\{\boldsymbol{%
h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}}\left(\left\{{h}_{n},\gamma_{n}%
\right\}_{n=1}^{N}\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="Ex4.m1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="Ex4.m2" class="ltx_Math" alttext="\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}{f_{\{\boldsymbol{h}_{n},%
\boldsymbol{\gamma}_{n}\}_{n=1}^{N}|\boldsymbol{w}}\left(\left\{{h}_{n},\gamma%
_{n}\right\}_{n=1}^{N}|w\right)\times f_{\boldsymbol{w}}(w)}" display="inline"><semantics><mrow><mrow><mrow><mrow><mi>arg</mi><mo lspace="0.167em">â¡</mo><mrow><munder><mi>max</mi><mrow><mi>w</mi><mo>âˆˆ</mo><msup><mi>â„</mi><msub><mi>M</mi><mi>w</mi></msub></msup></mrow></munder><mo lspace="0.167em">â¡</mo><msub><mi>f</mi><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>ğ’‰</mi><mi>n</mi></msub><mo>,</mo><msub><mi>ğœ¸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo fence="false">|</mo><mi>ğ’˜</mi></mrow></msub></mrow></mrow><mo>â¢</mo><mrow><mo>(</mo><mrow><msubsup><mrow><mo>{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>Î³</mi><mi>n</mi></msub><mo>}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo fence="false">|</mo><mi>w</mi></mrow><mo rspace="0.055em">)</mo></mrow></mrow><mo rspace="0.222em">Ã—</mo><msub><mi>f</mi><mi>ğ’˜</mi></msub></mrow><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}{f_{\{\boldsymbol{h}_{n},%
\boldsymbol{\gamma}_{n}\}_{n=1}^{N}|\boldsymbol{w}}\left(\left\{{h}_{n},\gamma%
_{n}\right\}_{n=1}^{N}|w\right)\times f_{\boldsymbol{w}}(w)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E4.m1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="E4.m2" class="ltx_Math" alttext="\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}\left(\prod_{n=1}^{N}f_{%
\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left({h}_{n},{\gamma}_{n}|w%
\right)\right)\times f_{\boldsymbol{w}}(w)" display="inline"><semantics><mrow><mrow><mi>arg</mi><mo lspace="0.167em">â¡</mo><mrow><mrow><munder><mi>max</mi><mrow><mi>w</mi><mo>âˆˆ</mo><msup><mi>â„</mi><msub><mi>M</mi><mi>w</mi></msub></msup></mrow></munder><mo>â¡</mo><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">âˆ</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>f</mi><mrow><mi>ğ’‰</mi><mo>,</mo><mrow><mi>ğœ¸</mi><mo fence="false">|</mo><mi>ğ’˜</mi></mrow></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><mrow><msub><mi>Î³</mi><mi>n</mi></msub><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace="0.055em">)</mo></mrow></mrow><mo rspace="0.222em">Ã—</mo><msub><mi>f</mi><mi>ğ’˜</mi></msub></mrow></mrow><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\arg\max_{w\in\mathds{R}^{M_{w}}}\left(\prod_{n=1}^{N}f_{%
\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left({h}_{n},{\gamma}_{n}|w%
\right)\right)\times f_{\boldsymbol{w}}(w)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">Following the same argument that led toÂ (<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:eq:minimize_log_likelihoods</span>), we arrive at:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E5.m1" class="ltx_Math" alttext="\displaystyle w^{\star}=" display="inline"><semantics><mrow><msup><mi>w</mi><mo>â‹†</mo></msup><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle w^{\star}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="E5.m2" class="ltx_Math" alttext="\displaystyle\&gt;\arg\min_{w\in\mathds{R}^{M}}\left\{-\frac{1}{N}\sum_{n=1}^{N}%
\log f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left({h}_{n},{%
\gamma}_{n}|w\right)-\frac{1}{N}\log f_{\boldsymbol{w}}(w)\right\}" display="inline"><semantics><mrow><mi>arg</mi><mo lspace="0.167em">â¡</mo><mrow><munder><mi>min</mi><mrow><mi>w</mi><mo>âˆˆ</mo><msup><mi>â„</mi><mi>M</mi></msup></mrow></munder><mo>â¡</mo><mrow><mo>{</mo><mrow><mrow><mo>âˆ’</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo>â¢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">âˆ‘</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mrow><mi>log</mi><mo lspace="0.167em">â¡</mo><msub><mi>f</mi><mrow><mi>ğ’‰</mi><mo>,</mo><mrow><mi>ğœ¸</mi><mo fence="false">|</mo><mi>ğ’˜</mi></mrow></mrow></msub></mrow><mo>â¢</mo><mrow><mo>(</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><mrow><msub><mi>Î³</mi><mi>n</mi></msub><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>âˆ’</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo lspace="0.167em">â¢</mo><mrow><mi>log</mi><mo lspace="0.167em">â¡</mo><msub><mi>f</mi><mi>ğ’˜</mi></msub></mrow><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\arg\min_{w\in\mathds{R}^{M}}\left\{-\frac{1}{N}\sum_{n=1}^{N}%
\log f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left({h}_{n},{%
\gamma}_{n}|w\right)-\frac{1}{N}\log f_{\boldsymbol{w}}(w)\right\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="Thmremark2" class="ltx_theorem ltx_theorem_remark">

<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem header-section-number"><span class="ltx_text ltx_font_italic">Remark 1.2</span></span><span class="ltx_text ltx_font_italic"> </span>(<span class="ltx_text ltx_font_bold">Distinction between the true model and the MAP estimate</span>)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div id="Thmremark2.p1" class="ltx_para">
<p class="ltx_p">Observe that we make a deliberate distinction between the <em class="ltx_emph ltx_font_italic">true</em> model <math id="Thmremark2.p1.m1" class="ltx_Math" alttext="w^{o}" display="inline"><semantics><msup><mi>w</mi><mi>o</mi></msup><annotation encoding="application/x-tex">w^{o}</annotation></semantics></math>, which parameterizes the distribution <math id="Thmremark2.p1.m2" class="ltx_Math" alttext="f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left(h,{\gamma}|w^{o}\right)" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>ğ’‰</mi><mo>,</mo><mrow><mi>ğœ¸</mi><mo fence="false">|</mo><mi>ğ’˜</mi></mrow></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><mi>h</mi><mo>,</mo><mrow><mi>Î³</mi><mo fence="false">|</mo><msup><mi>w</mi><mi>o</mi></msup></mrow><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left(h,{\gamma}|w^{o}\right)</annotation></semantics></math> from which the samples <math id="Thmremark2.p1.m3" class="ltx_Math" alttext="\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>ğ’‰</mi><mi>n</mi></msub><mo>,</mo><msub><mi>ğœ¸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}</annotation></semantics></math> are generated, and the MAP <em class="ltx_emph ltx_font_italic">estimate</em> <math id="Thmremark2.p1.m4" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math>, which maximizes the posterior distribution of <math id="Thmremark2.p1.m5" class="ltx_Math" alttext="{\boldsymbol{w}}" display="inline"><semantics><mi>ğ’˜</mi><annotation encoding="application/x-tex">{\boldsymbol{w}}</annotation></semantics></math> after observing the samples <math id="Thmremark2.p1.m6" class="ltx_Math" alttext="\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>ğ’‰</mi><mi>n</mi></msub><mo>,</mo><msub><mi>ğœ¸</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\boldsymbol{h}_{n},\boldsymbol{\gamma}_{n}\}_{n=1}^{N}</annotation></semantics></math>. In general, there will be a difference between the MAP model <math id="Thmremark2.p1.m7" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> and the true model <math id="Thmremark2.p1.m8" class="ltx_Math" alttext="w^{o}" display="inline"><semantics><msup><mi>w</mi><mi>o</mi></msup><annotation encoding="application/x-tex">w^{o}</annotation></semantics></math>. The expectation is that as the size of the data set <math id="Thmremark2.p1.m9" class="ltx_Math" alttext="N" display="inline"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> grows, the MAP model <math id="Thmremark2.p1.m10" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> will become increasingly accurate and approach <math id="Thmremark2.p1.m11" class="ltx_Math" alttext="w^{o}" display="inline"><semantics><msup><mi>w</mi><mi>o</mi></msup><annotation encoding="application/x-tex">w^{o}</annotation></semantics></math> as <math id="Thmremark2.p1.m12" class="ltx_Math" alttext="N\to\infty" display="inline"><semantics><mrow><mi>N</mi><mo stretchy="false">â†’</mo><mi mathvariant="normal">âˆ</mi></mrow><annotation encoding="application/x-tex">N\to\infty</annotation></semantics></math>. We will verify that this is the case further below. âˆ</p>
</div>
</div>
<div id="SS1.p3" class="ltx_para">
<p class="ltx_p">Note that in order to be able to <em class="ltx_emph ltx_font_italic">learn</em> an optimal parameterization <math id="SS1.p3.m1" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> according toÂ (<a href="#E5" title="In 1.1. From Inference to Learning â€£ 1. BAYESIAN INFERENCE â€£ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), we need to be able to collect a batch of feature-label pairs <math id="SS1.p3.m2" class="ltx_Math" alttext="\{h_{n},\gamma_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>Î³</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{h_{n},\gamma_{n}\}_{n=1}^{N}</annotation></semantics></math>. In the context of machine learning this step is frequently referred to as <em class="ltx_emph ltx_font_italic">training</em>, and the labeled data <math id="SS1.p3.m3" class="ltx_Math" alttext="\{h_{n},\gamma_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>Î³</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{h_{n},\gamma_{n}\}_{n=1}^{N}</annotation></semantics></math> is referred to as the <em class="ltx_emph ltx_font_italic">training data</em>. Once <math id="SS1.p3.m4" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> is determined, we can use the learned parameterization <math id="SS1.p3.m5" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> on an unlabeled feature vector <math id="SS1.p3.m6" class="ltx_Math" alttext="h^{\mathrm{test}}" display="inline"><semantics><msup><mi>h</mi><mi>test</mi></msup><annotation encoding="application/x-tex">h^{\mathrm{test}}</annotation></semantics></math>, to compute an approximate MAP estimate for its label:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx6" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E6.m1" class="ltx_Math" alttext="\displaystyle{\gamma^{\mathrm{test}}}^{\star}\triangleq\arg\max_{{\gamma}\in%
\Gamma}f_{\boldsymbol{\gamma}|\boldsymbol{h},\boldsymbol{w}}(\gamma|h^{\mathrm%
{test}},w^{\star})" display="inline"><semantics><mrow><mmultiscripts><mi>Î³</mi><mrow></mrow><mi>test</mi><mrow></mrow><mo>â‹†</mo></mmultiscripts><mo>â‰œ</mo><mrow><mrow><mi>arg</mi><mo lspace="0.167em">â¡</mo><mrow><munder><mi>max</mi><mrow><mi>Î³</mi><mo>âˆˆ</mo><mi mathvariant="normal">Î“</mi></mrow></munder><mo lspace="0.167em">â¡</mo><msub><mi>f</mi><mrow><mi>ğœ¸</mi><mo fence="false">|</mo><mrow><mi>ğ’‰</mi><mo>,</mo><mi>ğ’˜</mi></mrow></mrow></msub></mrow></mrow><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Î³</mi><mo fence="false">|</mo><mrow><msup><mi>h</mi><mi>test</mi></msup><mo>,</mo><msup><mi>w</mi><mo>â‹†</mo></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle{\gamma^{\mathrm{test}}}^{\star}\triangleq\arg\max_{{\gamma}\in%
\Gamma}f_{\boldsymbol{\gamma}|\boldsymbol{h},\boldsymbol{w}}(\gamma|h^{\mathrm%
{test}},w^{\star})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">We emphasize thatÂ (<a href="#E6" title="In 1.1. From Inference to Learning â€£ 1. BAYESIAN INFERENCE â€£ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) is only an <em class="ltx_emph ltx_font_italic">approximate</em> MAP estimate for the true label <math id="SS1.p3.m7" class="ltx_Math" alttext="{\gamma^{\mathrm{test}}}" display="inline"><semantics><msup><mi>Î³</mi><mi>test</mi></msup><annotation encoding="application/x-tex">{\gamma^{\mathrm{test}}}</annotation></semantics></math>, since it is generated using an estimate of the posterior distribution <math id="SS1.p3.m8" class="ltx_Math" alttext="f_{\boldsymbol{\gamma}|\boldsymbol{h},\boldsymbol{w}}(\gamma|h^{\mathrm{test}}%
,w^{\star})" display="inline"><semantics><mrow><msub><mi>f</mi><mrow><mi>ğœ¸</mi><mo fence="false">|</mo><mrow><mi>ğ’‰</mi><mo>,</mo><mi>ğ’˜</mi></mrow></mrow></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Î³</mi><mo fence="false">|</mo><mrow><msup><mi>h</mi><mi>test</mi></msup><mo>,</mo><msup><mi>w</mi><mo>â‹†</mo></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{\gamma}|\boldsymbol{h},\boldsymbol{w}}(\gamma|h^{\mathrm{test}}%
,w^{\star})</annotation></semantics></math> using the learned parameterizationÂ <math id="SS1.p3.m9" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math>. In general, and in particular for finite sample sizes <math id="SS1.p3.m10" class="ltx_Math" alttext="N" display="inline"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> of the training data, the learned parameterization <math id="SS1.p3.m11" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> will be different from the true parameterization <math id="SS1.p3.m12" class="ltx_Math" alttext="w^{o}" display="inline"><semantics><msup><mi>w</mi><mi>o</mi></msup><annotation encoding="application/x-tex">w^{o}</annotation></semantics></math> that actually generated the data. The difference between predictions made using the true model <math id="SS1.p3.m13" class="ltx_Math" alttext="w^{o}" display="inline"><semantics><msup><mi>w</mi><mi>o</mi></msup><annotation encoding="application/x-tex">w^{o}</annotation></semantics></math> and the learned model <math id="SS1.p3.m14" class="ltx_Math" alttext="w^{\star}" display="inline"><semantics><msup><mi>w</mi><mo>â‹†</mo></msup><annotation encoding="application/x-tex">w^{\star}</annotation></semantics></math> is known as a <em class="ltx_emph ltx_font_italic">generalization error</em>.</p>
</div>
<div id="Thmremark3" class="ltx_theorem ltx_theorem_remark">

<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem header-section-number"><span class="ltx_text ltx_font_italic">Remark 1.3</span></span><span class="ltx_text ltx_font_italic"> </span>(<span class="ltx_text ltx_font_bold">Compact notation for feature-label pairs</span>)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div id="Thmremark3.p1" class="ltx_para">
<p class="ltx_p">As is evident fromÂ (<a href="#E5" title="In 1.1. From Inference to Learning â€£ 1. BAYESIAN INFERENCE â€£ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), during learning, we are provided with feature-label pairs <math id="Thmremark3.p1.m1" class="ltx_Math" alttext="\{\boldsymbol{h},\boldsymbol{\gamma}\}" display="inline"><semantics><mrow><mo stretchy="false">{</mo><mi>ğ’‰</mi><mo>,</mo><mi>ğœ¸</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\boldsymbol{h},\boldsymbol{\gamma}\}</annotation></semantics></math> or their realizations <math id="Thmremark3.p1.m2" class="ltx_Math" alttext="\{h_{n},\gamma_{n}\}_{n=1}^{N}" display="inline"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>Î³</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{h_{n},\gamma_{n}\}_{n=1}^{N}</annotation></semantics></math>. To simplify the notation, we will collect features <math id="Thmremark3.p1.m3" class="ltx_Math" alttext="\boldsymbol{h}\in\mathds{R}^{M_{\boldsymbol{h}}}" display="inline"><semantics><mrow><mi>ğ’‰</mi><mo>âˆˆ</mo><msup><mi>â„</mi><msub><mi>M</mi><mi>ğ’‰</mi></msub></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{h}\in\mathds{R}^{M_{\boldsymbol{h}}}</annotation></semantics></math> and labels <math id="Thmremark3.p1.m4" class="ltx_Math" alttext="\boldsymbol{\gamma}\in\mathds{R}^{M_{\boldsymbol{\gamma}}}" display="inline"><semantics><mrow><mi>ğœ¸</mi><mo>âˆˆ</mo><msup><mi>â„</mi><msub><mi>M</mi><mi>ğœ¸</mi></msub></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{\gamma}\in\mathds{R}^{M_{\boldsymbol{\gamma}}}</annotation></semantics></math> into a single augmented data vector <math id="Thmremark3.p1.m5" class="ltx_Math" alttext="{\boldsymbol{x}}\in\mathds{R}^{M_{\boldsymbol{h}}+M_{\boldsymbol{\gamma}}}" display="inline"><semantics><mrow><mi>ğ’™</mi><mo>âˆˆ</mo><msup><mi>â„</mi><mrow><msub><mi>M</mi><mi>ğ’‰</mi></msub><mo>+</mo><msub><mi>M</mi><mi>ğœ¸</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">{\boldsymbol{x}}\in\mathds{R}^{M_{\boldsymbol{h}}+M_{\boldsymbol{\gamma}}}</annotation></semantics></math>, such that:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx7" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(7)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E7.m1" class="ltx_Math" alttext="\displaystyle{\boldsymbol{x}}\triangleq\mathrm{col}\left\{\boldsymbol{h},%
\boldsymbol{\gamma}\right\}=\begin{pmatrix}\boldsymbol{h}\\
\boldsymbol{\gamma}\end{pmatrix}" display="inline"><semantics><mrow><mi>ğ’™</mi><mo>â‰œ</mo><mrow><mi>col</mi><mo>â¢</mo><mrow><mo>{</mo><mi>ğ’‰</mi><mo>,</mo><mi>ğœ¸</mi><mo>}</mo></mrow></mrow><mo>=</mo><mrow><mo>(</mo><mtable rowspacing="0pt"><mtr><mtd><mi>ğ’‰</mi></mtd></mtr><mtr><mtd><mi>ğœ¸</mi></mtd></mtr></mtable><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle{\boldsymbol{x}}\triangleq\mathrm{col}\left\{\boldsymbol{h},%
\boldsymbol{\gamma}\right\}=\begin{pmatrix}\boldsymbol{h}\\
\boldsymbol{\gamma}\end{pmatrix}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">Similarly, we will collect realizations into <math id="Thmremark3.p1.m6" class="ltx_Math" alttext="x_{n}\triangleq\mathrm{col}\left\{h_{n},\gamma_{n}\right\}" display="inline"><semantics><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>â‰œ</mo><mrow><mi>col</mi><mo>â¢</mo><mrow><mo>{</mo><msub><mi>h</mi><mi>n</mi></msub><mo>,</mo><msub><mi>Î³</mi><mi>n</mi></msub><mo>}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">x_{n}\triangleq\mathrm{col}\left\{h_{n},\gamma_{n}\right\}</annotation></semantics></math>. In this manner, we can write more compactly:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx8" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(8)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E8.m1" class="ltx_Math" alttext="\displaystyle f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left({h},{%
\gamma}|w\right)=f_{{\boldsymbol{x}}|\boldsymbol{w}}\left(x|w\right)" display="inline"><semantics><mrow><mrow><msub><mi>f</mi><mrow><mi>ğ’‰</mi><mo>,</mo><mrow><mi>ğœ¸</mi><mo fence="false">|</mo><mi>ğ’˜</mi></mrow></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><mi>h</mi><mo>,</mo><mrow><mi>Î³</mi><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>f</mi><mrow><mi>ğ’™</mi><mo fence="false">|</mo><mi>ğ’˜</mi></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle f_{\boldsymbol{h},\boldsymbol{\gamma}|\boldsymbol{w}}\left({h},{%
\gamma}|w\right)=f_{{\boldsymbol{x}}|\boldsymbol{w}}\left(x|w\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<p class="ltx_p">The MAP learning problemÂ (<a href="#E5" title="In 1.1. From Inference to Learning â€£ 1. BAYESIAN INFERENCE â€£ Lecture 1: Foundations in Deterministic and Stochastic Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) then becomes:</p>
<div style="max-width: 100%; overflow-x: auto; overflow-y: hidden;">
<table id="EGx9" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(9)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="E9.m1" class="ltx_Math" alttext="\displaystyle w^{\star}=" display="inline"><semantics><mrow><msup><mi>w</mi><mo>â‹†</mo></msup><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle w^{\star}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="E9.m2" class="ltx_Math" alttext="\displaystyle\&gt;\arg\min_{w\in\mathds{R}^{M}}\left\{-\frac{1}{N}\sum_{n=1}^{N}%
\log f_{{\boldsymbol{x}}|\boldsymbol{w}}\left(x_{n}|w\right)-\frac{1}{N}\log f%
_{\boldsymbol{w}}(w)\right\}" display="inline"><semantics><mrow><mi>arg</mi><mo lspace="0.167em">â¡</mo><mrow><munder><mi>min</mi><mrow><mi>w</mi><mo>âˆˆ</mo><msup><mi>â„</mi><mi>M</mi></msup></mrow></munder><mo>â¡</mo><mrow><mo>{</mo><mrow><mrow><mo>âˆ’</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo>â¢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">âˆ‘</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mrow><mi>log</mi><mo lspace="0.167em">â¡</mo><msub><mi>f</mi><mrow><mi>ğ’™</mi><mo fence="false">|</mo><mi>ğ’˜</mi></mrow></msub></mrow><mo>â¢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>n</mi></msub><mo fence="false">|</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>âˆ’</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo lspace="0.167em">â¢</mo><mrow><mi>log</mi><mo lspace="0.167em">â¡</mo><msub><mi>f</mi><mi>ğ’˜</mi></msub></mrow><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\&gt;\arg\min_{w\in\mathds{R}^{M}}\left\{-\frac{1}{N}\sum_{n=1}^{N}%
\log f_{{\boldsymbol{x}}|\boldsymbol{w}}\left(x_{n}|w\right)-\frac{1}{N}\log f%
_{\boldsymbol{w}}(w)\right\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
</div>
</div>
</section>
</div>
</section>
</div>
</div>
</div>
<a href="index.html" class="navigation navigation-prev" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
</div>
</div>
<script src="bookml/gitbook/js/app.min.js"></script>
<script src="bookml/gitbook/js/plugin-fontsettings.js"></script>
<script src="bookml/gitbook/js/plugin-bookdown.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
        MathJax = {
          startup: {
            ready() {
              // do not process equations disabled with \bmlDisableMathJax (code suggested by Davide P. Cervone)
              class bmlFindMathML extends MathJax._.input.mathml.FindMathML.FindMathML {
                processMath(set) {
                  const adaptor = this.adaptor;
                  for (const node of set.values()) {
                    if (adaptor.hasClass(node, 'bml_disable_mathjax')) {
                      set.delete(node);
                    }
                  }
                  return super.processMath(set);
                }
              }

              MathJax._.components.global.combineDefaults(MathJax.config, 'mml', {FindMathML: new bmlFindMathML()});

              MathJax.startup.defaultReady();

              // preproces MathML to make MathJax aware of certain LaTeXML and BookML additional info
              const mmlFilters = MathJax.startup.input[0].mmlFilters;

              // convert the LaTeXML calligraphic (chancery) annotation to a form MathJax understands
              // since the corresponding Unicode characters render as script (rounded)
              const script2latin = {
                'ğ’œ': 'A', 'â„¬': 'B', 'ğ’': 'C', 'ğ’Ÿ': 'D', 'â„°': 'E', 'â„±': 'F', 'ğ’¢': 'G',
                'â„‹': 'H', 'â„': 'I', 'ğ’¥': 'J', 'ğ’¦': 'K', 'â„’': 'L', 'â„³': 'M', 'ğ’©': 'N',
                'ğ’ª': 'O', 'ğ’«': 'P', 'ğ’¬': 'Q', 'â„›': 'R', 'ğ’®': 'S', 'ğ’¯': 'T', 'ğ’°': 'U',
                'ğ’±': 'V', 'ğ’²': 'W', 'ğ’³': 'X', 'ğ’´': 'Y', 'ğ’µ': 'Z',
              };

              mmlFilters.add((args) => {
                for (const n of args.data.getElementsByClassName('ltx_font_mathcaligraphic')) {
                  n.classList.add('MJX-tex-calligraphic');
                  const letter = script2latin[n.textContent];
                  if (letter !== undefined) { n.textContent = letter; }
                }
              });

              // adjust characters based on Unicode variation sequences
              const replacements = {
                // MathJax renders the empty set as the U+FE00 variant, so the plain character needs adjusting
                'âˆ…': { variant: 'variant' },
                // MathJax renders script characters in rounded style, which is fine for no variation and U+FE00
                'ğ’œ\xFE00': { text: 'A', variant: 'tex-calligraphic' },
                'â„¬\xFE00': { text: 'B', variant: 'tex-calligraphic' },
                'ğ’\xFE00': { text: 'C', variant: 'tex-calligraphic' },
                'ğ’Ÿ\xFE00': { text: 'D', variant: 'tex-calligraphic' },
                'â„°\xFE00': { text: 'E', variant: 'tex-calligraphic' },
                'â„±\xFE00': { text: 'F', variant: 'tex-calligraphic' },
                'ğ’¢\xFE00': { text: 'G', variant: 'tex-calligraphic' },
                'â„‹\xFE00': { text: 'H', variant: 'tex-calligraphic' },
                'â„\xFE00': { text: 'I', variant: 'tex-calligraphic' },
                'ğ’¥\xFE00': { text: 'J', variant: 'tex-calligraphic' },
                'ğ’¦\xFE00': { text: 'K', variant: 'tex-calligraphic' },
                'â„’\xFE00': { text: 'L', variant: 'tex-calligraphic' },
                'â„³\xFE00': { text: 'M', variant: 'tex-calligraphic' },
                'ğ’©\xFE00': { text: 'N', variant: 'tex-calligraphic' },
                'ğ’ª\xFE00': { text: 'O', variant: 'tex-calligraphic' },
                'ğ’«\xFE00': { text: 'P', variant: 'tex-calligraphic' },
                'ğ’¬\xFE00': { text: 'Q', variant: 'tex-calligraphic' },
                'â„›\xFE00': { text: 'R', variant: 'tex-calligraphic' },
                'ğ’®\xFE00': { text: 'S', variant: 'tex-calligraphic' },
                'ğ’¯\xFE00': { text: 'T', variant: 'tex-calligraphic' },
                'ğ’°\xFE00': { text: 'U', variant: 'tex-calligraphic' },
                'ğ’±\xFE00': { text: 'V', variant: 'tex-calligraphic' },
                'ğ’²\xFE00': { text: 'W', variant: 'tex-calligraphic' },
                'ğ’³\xFE00': { text: 'X', variant: 'tex-calligraphic' },
                'ğ’´\xFE00': { text: 'Y', variant: 'tex-calligraphic' },
                'ğ’µ\xFE00': { text: 'Z', variant: 'tex-calligraphic' }
              };

              mmlFilters.add((args) => {
                let nodes = document.evaluate('.//m:mi | .//m:mn | .//m:mo | .//m:ms', args.data,
                  () => 'http://www.w3.org/1998/Math/MathML', XPathResult.UNORDERED_NODE_SNAPSHOT_TYPE);
                for (let i = 0; i < nodes.snapshotLength; i++) {
                  const n = nodes.snapshotItem(i);
                  const repl = replacements[n.innerHTML];
                  if (repl !== undefined) {
                    const variant = repl['variant'];
                    const text = repl['text'];
                    if (variant !== undefined) { n.classList.add('MJX-' + variant); n.removeAttribute('mathvariant'); }
                    if (text !== undefined) { n.innerHTML = text; }
                  }
                }
              });
            }
          }
        };
      </script>
<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js"></script>
<script type="text/javascript">
              gitbook.require(["gitbook"], function(gitbook) {
              gitbook.start({
                "fontsettings": {
                  "theme": "white",
                  "family": "sans",
                  "size": 2
                },
                "download": [ [ "lecture1.pdf", "PDF (serif)" ], ],
                  "search": false,
                  "toc": {
                    "collapse": "none"
                  }
                });
              });
            </script>
</body>

</html>
